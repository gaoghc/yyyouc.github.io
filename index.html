<!DOCTYPE html>
<html>
<head>
  <meta charset="utf-8">
  
  <title>YYYOUC</title>
  <meta name="viewport" content="width=device-width, initial-scale=1, maximum-scale=1">
  <meta name="description">
<meta property="og:type" content="website">
<meta property="og:title" content="YYYOUC">
<meta property="og:url" content="http://yoursite.com/index.html">
<meta property="og:site_name" content="YYYOUC">
<meta property="og:description">
<meta name="twitter:card" content="summary">
<meta name="twitter:title" content="YYYOUC">
<meta name="twitter:description">
  
    <link rel="alternative" href="/atom.xml" title="YYYOUC" type="application/atom+xml">
  
  
    <link rel="icon" href="/favicon.png">
  
  <link href="//fonts.googleapis.com/css?family=Source+Code+Pro" rel="stylesheet" type="text/css">
  <link rel="stylesheet" href="/css/style.css" type="text/css">
  

</head>
<body>
  <div id="container">
    <div id="wrap">
      <header id="header">
  <div id="banner"></div>
  <div id="header-outer" class="outer">
    <div id="header-title" class="inner">
      <h1 id="logo-wrap">
        <a href="/" id="logo">YYYOUC</a>
      </h1>
      
    </div>
    <div id="header-inner" class="inner">
      <nav id="main-nav">
        <a id="main-nav-toggle" class="nav-icon"></a>
        
          <a class="main-nav-link" href="/">Home</a>
        
          <a class="main-nav-link" href="/archives">Archives</a>
        
      </nav>
      <nav id="sub-nav">
        
          <a id="nav-rss-link" class="nav-icon" href="/atom.xml" title="RSS Feed"></a>
        
        <a id="nav-search-btn" class="nav-icon" title="Search"></a>
      </nav>
      <div id="search-form-wrap">
        <form action="//google.com/search" method="get" accept-charset="UTF-8" class="search-form"><input type="search" name="q" results="0" class="search-form-input" placeholder="Search"><button type="submit" class="search-form-submit">&#xF002;</button><input type="hidden" name="q" value="site:http://yoursite.com"></form>
      </div>
    </div>
  </div>
</header>
      <div class="outer">
        <section id="main">
  
    <article id="post-Density-Estimation" class="article article-type-post" itemscope itemprop="blogPost">
  <div class="article-meta">
    <a href="/2015/07/20/Density-Estimation/" class="article-date">
  <time datetime="2015-07-21T00:21:14.000Z" itemprop="datePublished">2015-07-20</time>
</a>
    
  </div>
  <div class="article-inner">
    
    
      <header class="article-header">
        
  
    <h1 itemprop="name">
      <a class="article-title" href="/2015/07/20/Density-Estimation/">Density Estimation</a>
    </h1>
  

      </header>
    
    <div class="article-entry" itemprop="articleBody">
      
        <h1 id="Introduction">Introduction</h1><p>We have talked about the <a href="Bayesian-Decision-Theory.md">Bayesian Decision Theory</a></p>

      
    </div>
    <footer class="article-footer">
      <a data-url="http://yoursite.com/2015/07/20/Density-Estimation/" data-id="cicclz2kf000838fffooy3ba6" class="article-share-link">Share</a>
      
        <a href="http://yoursite.com/2015/07/20/Density-Estimation/#disqus_thread" class="article-comment-link">Comments</a>
      
      
  <ul class="article-tag-list"><li class="article-tag-list-item"><a class="article-tag-list-link" href="/tags/Machine-Learning/">Machine Learning</a></li></ul>

    </footer>
  </div>
  
</article>


  
    <article id="post-Bayesian-Decision-Theory" class="article article-type-post" itemscope itemprop="blogPost">
  <div class="article-meta">
    <a href="/2015/07/18/Bayesian-Decision-Theory/" class="article-date">
  <time datetime="2015-07-19T01:38:12.000Z" itemprop="datePublished">2015-07-18</time>
</a>
    
  </div>
  <div class="article-inner">
    
    
      <header class="article-header">
        
  
    <h1 itemprop="name">
      <a class="article-title" href="/2015/07/18/Bayesian-Decision-Theory/">Bayesian Decision Theory</a>
    </h1>
  

      </header>
    
    <div class="article-entry" itemprop="articleBody">
      
        <h2 id="Introduction">Introduction</h2><p>Bayesian decision theory is a fundamental statistical method to the problem of pattern classification. It is to use the probability to perform classification decision. It has two assumptions:</p>
<ul>
<li>the decision problem is posed in probilistic terms;</li>
<li>all the involved probability values are known.</li>
</ul>
<h2 id="Bayes_Formula">Bayes Formula</h2><p>Bayes formula is the basis of the Bayesian decision theory. It is defined as follows:<br>$$<br>P(w_j|x)=\frac{p(x|w_j)P(w_j)}{p(x)}<br>$$<br>where $P(w_j|x)$ is the <strong>posterior probability</strong>, $P(w_j)$ is the <strong>prior probability</strong> and $p(x|w_j)$ is the <strong>conditional probability density</strong>. And $p(x)=\sum_{j=1}^{n}p(x|w_j)P(w_j)$.<br>But what is the meaning of prior probability, posterior probability and the conditional probability density?</p>
<ul>
<li>Prior probability is the assumption that we give to a problem. It may be correct or not. However, we can use conditional probability density to justify it.</li>
<li>Conditional probability density is the likelihood of $w_j$ with respect to $x$. If it is large, it means that $w_j$ is more “likely” to be the true category.</li>
<li>Posterior probability is the justified assumption. </li>
</ul>
<p>In fact, given a data point, we can give an assumption about the category which it belongs to. Such assumption maybe right or not. However, such a data point has some features. Therefore, under such a assumption, we can use these features of the data point to justify whether the assumtion is proper. If so, we will get a large posterior probability. Otherwise, we will get a small one.<br>This is the basic idea of Bayes formula. It can be interpreted as follows:<br>$$<br>posterior = \frac{likelihood\times prior}{evidence}<br>$$<br>This formula shows that by observing the value of $x$ we can convert the prior probability $P(w_j)$to the a  posterior probability $P(w_j|x)$.</p>
<h2 id="Loss_Function">Loss Function</h2><p>Now we can use Bayes formula to give decision about the classification. But how to measure the cost of our decision?</p>
<p>Since $P(w_j|x)$ is the probability that the true state of nature is $w_j$ , the expected loss associated with taking action $\alpha_i$ is<br>$$<br>L(\alpha_i|x)=\sum_{j=1}^{c}I(\alpha_i|w_j)P(w_j|x)<br>$$<br>In decision-theoretic terminology, an expected loss is called a <strong>risk</strong>, and $L(\alpha_i|x)$ is called the <strong>conditional risk</strong>. Whenever we encounter a particular observation $x$, we can minimize our expected loss by selecting the action that minimizes the conditional risk. Since $L(\alpha_i|x)$ is the conditional risk, <strong>overall risk</strong> is given by<br>$$<br>L=\int L(\alpha(x)|x)p(x)dx<br>$$<br>Clearly, if $\alpha(x)$ is chosen such that $L(\alpha(x))$ is as small as possible for every $x$, then the overall risk will be minimized. This justifies the following statement of the Bayes decision rule: To minimize the overall risk, compute the conditional risk<br>$$<br>L(\alpha_i|x)=\sum_{j=1}^{c}I(\alpha_i|w_j)P(w_j|x)<br>$$<br>and select the action $\alpha_i$ for which $L(\alpha_i|x)$ is minimum. The resulting minimum overall risk is called the <strong>Bayes risk</strong>.</p>
<p>Now, let us define a function<br>$$<br>I(\alpha_j|w_i)=\left\{<br>\begin{aligned}<br>0 &amp;&amp; i=j \\<br>1 &amp;&amp; i \neq j \\<br>\end{aligned}<br>\right.<br>$$<br>Then, the conditional risk can be defined as follows.<br>$$<br>L(\alpha_i|x)=\sum_{j=1}^{c}I(\alpha_i|w_j)P(w_j|x) \<br>= \sum_{j\neq i}P(w_j|x) \<br>= 1-P(w_i|x)<br>$$<br>From the definition of the loss function, we can conclude that minimizing loss function $L(\alpha_i|x)$ is to maximize the posterior probability $P(w_j|x)$. In other words, decide $w_i$ if $P(w_i|x)&gt;P(w_j|x)$ for all $j\neq i$</p>
<h2 id="Bayes_Classifier">Bayes Classifier</h2><p>There are many different ways to represent pattern classifiers. One of the most useful is in terms of a set of <strong>discriminant functions</strong> $g_i(x), i = 1\cdots c$. The classifier is said to assign a data point $x$ to class $w_i$ if for all $j\neq i$<br>$$<br>g_i(x)&gt;g_j(x)<br>$$</p>
<p>A Bayes classifier is easily and naturally represented in this way. For the general case with risks, we can let $g_i(x) = -L(\alpha_i|x)$, since the maximum discriminant function will then correspond to the minimum conditional risk. For the minimum-error-rate case, we can simplify things further by taking $g_i(x) = P(w_j|x)$, so that the maximum discriminant function corresponds to the maximum posterior probability.</p>
<p>Clearly, the choice of discriminant functions is not unique. We can always multiply all the discriminant functions by the same positive constant or shift them by the same additive constant without influencing the decision. More generally, if we replace every $g_i(x)$ by $f(g_i(x))$, where $f(.)$ is a monotonically increasing function, the resulting classification is unchanged. This observation can lead to significant analytical and computational simplifications. In particular, for minimum-error-rate classification, any of the following choices gives identical classification results, but some can be<br>much simpler to understand or to compute than others:<br>$$<br>g_i(x)=P(w_i|x) \\<br>g_i(x) = p(x|w_i)P(w_i) \\<br>g_i(x) = lnp(x|w_i)+lnP(w_i)<br>$$</p>
<p>Even though the discriminant functions can be written in a variety of forms, the decision rules are equivalent. The effect of any decision rule is to divide the feature space into $c$ decision regions, $R_1,\cdots, R_c$. If $g_i(x) &gt; g_j(x)$ for all $j\neq i$, then $x$ is in region $R_i$, and the decision rule calls for us to assign $x$ to $w_i$. The regions are separated<br>by decision boundaries, surfaces in feature space where ties occur among the largest discriminant functions.</p>
<h2 id="Naïve_Bayes_Method">Naïve Bayes Method</h2><p>Naïve Bayes is a subset of Bayesian decision theory. It uses the Bayesian decision theory to do classification. Here, Naïve means the features of the data point are independent to each other.</p>
<p>Given a data point $x=(x_1, x_2,\cdots,x_n)$ where $x_i$ is the $i$-th feature of the data point, and given classes $c_1, c_2, \cdots, c_k$. If $P(c_i|x)&gt;P(c_j|x)$ for all $j\neq i$, then data point $x$ belongs to class $c_i$. Specifically,<br>$$<br>P(c_i|x)=\frac{p(x|c_i)P(c_i)}{p(x)}<br>$$<br>Due to the independence of the features, $p(x|c_i)=p(x_1|c_i)p(x_2|c_i)\cdots p(x_n|c_i)$.</p>
<p>This is the basic idea of Naïve Bayes method.</p>

      
    </div>
    <footer class="article-footer">
      <a data-url="http://yoursite.com/2015/07/18/Bayesian-Decision-Theory/" data-id="cicclz2j4000038ffv65p1aa3" class="article-share-link">Share</a>
      
        <a href="http://yoursite.com/2015/07/18/Bayesian-Decision-Theory/#disqus_thread" class="article-comment-link">Comments</a>
      
      
  <ul class="article-tag-list"><li class="article-tag-list-item"><a class="article-tag-list-link" href="/tags/Machine-Learning/">Machine Learning</a></li></ul>

    </footer>
  </div>
  
</article>


  
    <article id="post-Support Vector Machine" class="article article-type-post" itemscope itemprop="blogPost">
  <div class="article-meta">
    <a href="/2015/07/07/Support Vector Machine/" class="article-date">
  <time datetime="2015-07-08T04:09:39.000Z" itemprop="datePublished">2015-07-07</time>
</a>
    
  </div>
  <div class="article-inner">
    
    
      <header class="article-header">
        
  
    <h1 itemprop="name">
      <a class="article-title" href="/2015/07/07/Support Vector Machine/">Support Vector Machine</a>
    </h1>
  

      </header>
    
    <div class="article-entry" itemprop="articleBody">
      
        <h2 id="Introduction">Introduction</h2><p>When it comes to classification methods, there are two categories. The first category is to construct a <strong>discriminant function</strong> for each class, and then classify each sample to the class with the largest value for its discriminant function, such as logistic regression method. The second method is to construct a <strong>separating hyperplane</strong>, such as perceptron method and SVM.</p>
<h2 id="Objective_function">Objective function</h2><p>SVM is to find an optimal separating hyperplane among different classes such that the <strong>margin</strong> is maximized, where margin is the smallest distance between the separating plane and any of the samples. </p>
<p>The objective function is as follows,<br>$$\max_{w,b} margin(w, b) \\<br>s.t. y_i(w^Tx_i+b)&gt;0$$<br>where $margin(w,b)=\min_{i=1,2,..,N}distance(x_i,w,b)$, and $distance(x_i, w, b)=\frac{|w^Tx_i+b|}{||w||}=\frac{y_i(w^Tx_i+b)}{||w||}$. The constraint means that we are only interested in the hyperplane for which all samples are correctly classified.</p>
<p>Due to the linearity of the hyperplane, there exists many representations for the same hyperplane, for example, $(kw)^Tx+kb=0$. To avoid such cases, we scale $w$ and $b$ such that $\min_{i=1,2,..,N} y_i(w^Tx_i+b)=1$. Thus, $margin(w,b)=\frac{1}{||w||}$. Thus, the objective function is as follows.<br>$$ \max_{w,b}\frac{1}{||w||} \\<br>s.t. y_i(w^Tx_i+b)\geq1 $$<br>It is equivalent to the following:<br>$$ \min_{w,b}\frac{1}{2}w^Tw \\<br>s.t. y_i(w^Tx_i+b)\geq1 $$</p>
<h2 id="Optimization_algorithm">Optimization algorithm</h2><p>There are two methods to solve this objective function. The first method is to solve the primal problem directly. The second method is to solve the dual problem.</p>
<h3 id="1-_Primal_problem">1. Primal problem</h3><p>This problem is a quadratic programming (QP) problem. It can be solved directly by many optimization tools. The general form of the QP problem is as follows.<br>$$\min_{u} \frac{1}{2}u^TQu+p^Tu \\<br>s.t.a_i^Tu\geq c_i , i=1,…,m<br>$$<br>The objective function of SVM can be represented as such form, where $u=\left[<br>  \begin{array}{c}<br>    b\\ w<br>  \end{array}<br>\right] $, $Q=\left[<br>  \begin{array}{cc}<br>    0 &amp; 0_d^T \\<br>    0_d &amp; I_d<br>  \end{array}<br>\right] $, $p=0_{d+1}$, $a_i^T=y_i[1,  x_i^T]$,$c_i=1$. For this method, the number of the variables is $d$ (the number of features), and the number of the constraints is $N$ (the number of samples). If $d$ is very large, especially for the kernel SVM, the computation will also be very large. How to address this problem? We can solve the dual problem rather than the primal problem.</p>
<h3 id="2-_Dual_problem">2. Dual problem</h3><h4 id="2-1_Duality">2.1 Duality</h4><p>At first, we will talk about some basic knowledge about the dual problem. We consider a standard optimization problem:<br>$$<br>\min f_0(x)  \\<br>s.t. f_i(x) \leq 0, i=1,…,m \\<br>h_i(x) = 0, i=1,…,p<br>$$<br>This is a constrained optimization problem. Ususally it is more difficult to solve than the unconstrained optimization problem. Can we transform it to an unconstrained one? Of course, we define a Lagrangian function as follows:<br>$$L(x,\lambda, v)=f_0(x)+\sum_{i=1}^m\lambda_i f_i(x)+\sum_{i=1}^pv_ih_i(x)$$<br>where $\lambda_i$ and $v_i$ is the Lagrangian multiplier. Note that we put the constraint into the objective function so that it becomes an uncontrained problem. However, are they equivalent?<br>Consider the following function:<br>\begin{equation}<br>\theta_p(x)=\max_{\lambda_i,v_i: \lambda_i\geq 0} L(x, \lambda, v)<br>\end{equation}<br>If some constraint are violated, such as $f_i(x)&gt;0$ or $h_i(x)\neq0$, we can adjust $\lambda_i$ and $v_i$ such that $\theta_p(x)$ approaches to infinity. If all the constraints are satisifed, it is easy to know that $\lambda_i$ should be equal to 0, and then $\theta_p(x)$ is equal to $f_0(x)$ exactly. Thus, we can minimize $\theta_p(x)$ to enforce the constraint to be satisfied, that is<br>\begin{equation}<br>\min_{x}\theta_p(x)=\min_{x} \max_{\lambda_i,v_i: \lambda_i\geq 0} L(x, \lambda, v)<br>\end{equation}<br>then it will be equivalent the standard form. This is the primal problem, in the following we will derive the dual problem. Note that for any $\lambda’$ and $v’$, we have $$\min_{x} \max_{\lambda_i,v_i: \lambda_i\geq 0} L(x, \lambda, v) \geq \min_{x} L(x, \lambda’, v’)$$ (because max $\geq$ any), and then $$\min_{x} \max_{\lambda_i,v_i: \lambda_i\geq 0} L(x, \lambda, v) \geq \max_{\lambda_i’,v_i’: \lambda_i’\geq 0}\min_{x} L(x, \lambda’, v’)$$ (because best is one of any).<br>The right side is the dual problem, and $\min_{x} L(x, \lambda, v)$ is the dual function. Note that the dual problem gives a lower bound to the primal problem. </p>
<p>When the solution $x^{<em>}$, $\lambda^{</em>}$ and $v^{*}$ satisfy the KKT conditions, they are the solution of both primal and dual problems.</p>
<h4 id="2-2_Dual_SVM">2.2 Dual SVM</h4><p>Based on the above foundation, the dual SVM is as follows<br>$$\max_{\alpha_i\geq 0}{\min_{b,w}\frac{1}{2}w^Tw+\sum_{i=1}^{N}\alpha_i(1-y_i(w^Tx_i+b))}$$</p>
<p>At first, we solve the inner problem<br> $$\min_{b,w}\frac{1}{2}w^Tw+\sum_{i=1}^{N}\alpha_i(1-y_i(w^Tx_i+b))$$<br> This is an unconstrained problem. It is easy to solve by setting the direvative with respect to $w$ and $b$ as zero. We can get<br> $$<br> \sum_{i=1}^{N}\alpha_iy_i=0 \\<br> \sum_{i=1}^{N}\alpha_iy_ix_i=w<br> $$<br> Then,<br>$$<br>\frac{1}{2}w^Tw+\sum_{i=1}^{N}\alpha_i(1-y_i(w^Tx_i+b)) \\<br>=\frac{1}{2}w^T\sum_{i=1}^{N}\alpha_iy_ix_i+\sum_{i=1}^{N}\alpha_i-w^T\sum_{i=1}^N\alpha_iy_ix_i-b\sum_{i=1}^N\alpha_iy_i \\<br>=\sum_{i=1}^{N}\alpha_i-\frac{1}{2}w^T\sum_{i=1}^{N}\alpha_iy_ix_i \\<br>= \sum_{i=1}^{N}\alpha_i-\frac{1}{2}(\sum_{i=1}^{N}\alpha_iy_ix_i)^T\sum_{i=1}^{N}\alpha_iy_ix_i \\<br>= \sum_{i=1}^{N}\alpha_i-\frac{1}{2}\sum_{i,j=1}^{N}\alpha_i\alpha_jy_iy_jx_i^Tx_j<br>$$<br>Now we have the dual problem as follows.<br>$$<br>\min \frac{1}{2}\sum_{i=1}^{N}\sum_{j=1}^{N}\alpha_i\alpha_jy_iy_jx_i^Tx_j-\sum_{i=1}^{N}\alpha_i \\<br>s.t. \alpha_i \geq 0, i=1,…,N \\<br> \sum_{i=1}^{N}\alpha_iy_i=0<br>$$<br>This is a QP with $N$ variables and $N+1$ constraints. Therefore, it can be easily solved by optimization tools. Note that when we solve this QP, the matrix $Q$ consists of $q_{ij}=y_iy_jx_i^Tx_j$ usually is dense. Thus, it needs large storage.</p>
<p>After we got the solution of the dual problem, how can we get the primal solution $w$ and $b$? Note that $w=\sum_{i=1}^{N}\alpha_iy_ix_i$. Whats more, according to the KKT dual complementary conditions,$\alpha_i(1-y_i(w^Tx_i+b))=0$, if $\alpha_i&gt;0$, $1-y_i(w^Tx_i+b)=0$, that is $b=y_i-w^Tx_i$. Note that $\alpha_i&gt;0$ means $y_i(w^Tx_i+b)=1$, thus this point locates on the boundary. Such points are called <strong>support vector</strong>. </p>
<p>After obtainning the optimal $w$ and $b$ from the trainning data, we can classify the test data by the following formula.<br>$$<br>w^Tx+b=(\sum_{i=1}^N\alpha_iy_ix_i)^Tx+b=\sum_{i=1}^N\alpha_iy_i x_i^Tx+b<br>$$<br>Note that we only need to calculate the inner product between the testing data and the support vector (corresponds to $\alpha_i&gt;0$) rather than the whole training data, thus it is efficient.</p>
<h2 id="Soft-Margin_SVM">Soft-Margin SVM</h2>
      
    </div>
    <footer class="article-footer">
      <a data-url="http://yoursite.com/2015/07/07/Support Vector Machine/" data-id="cicclz2kd000638ff8k8m8048" class="article-share-link">Share</a>
      
        <a href="http://yoursite.com/2015/07/07/Support Vector Machine/#disqus_thread" class="article-comment-link">Comments</a>
      
      
  <ul class="article-tag-list"><li class="article-tag-list-item"><a class="article-tag-list-link" href="/tags/Machine-Learning/">Machine Learning</a></li></ul>

    </footer>
  </div>
  
</article>


  
    <article id="post-hello-world" class="article article-type-post" itemscope itemprop="blogPost">
  <div class="article-meta">
    <a href="/2015/05/17/hello-world/" class="article-date">
  <time datetime="2015-05-18T01:29:05.546Z" itemprop="datePublished">2015-05-17</time>
</a>
    
  </div>
  <div class="article-inner">
    
    
      <header class="article-header">
        
  
    <h1 itemprop="name">
      <a class="article-title" href="/2015/05/17/hello-world/">Hello World</a>
    </h1>
  

      </header>
    
    <div class="article-entry" itemprop="articleBody">
      
        <p>Welcome to <a href="http://hexo.io/" target="_blank" rel="external">Hexo</a>! This is your very first post. Check <a href="http://hexo.io/docs/" target="_blank" rel="external">documentation</a> for more info. If you get any problems when using Hexo, you can find the answer in <a href="http://hexo.io/docs/troubleshooting.html" target="_blank" rel="external">troubleshooting</a> or you can ask me on <a href="https://github.com/hexojs/hexo/issues" target="_blank" rel="external">GitHub</a>.</p>
<h2 id="Quick_Start">Quick Start</h2><h3 id="Create_a_new_post">Create a new post</h3><figure class="highlight bash"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">$ hexo new <span class="string">"My New Post"</span></span><br></pre></td></tr></table></figure>
<p>More info: <a href="http://hexo.io/docs/writing.html" target="_blank" rel="external">Writing</a></p>
<h3 id="Run_server">Run server</h3><figure class="highlight bash"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">$ hexo server</span><br></pre></td></tr></table></figure>
<p>More info: <a href="http://hexo.io/docs/server.html" target="_blank" rel="external">Server</a></p>
<h3 id="Generate_static_files">Generate static files</h3><figure class="highlight bash"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">$ hexo generate</span><br></pre></td></tr></table></figure>
<p>More info: <a href="http://hexo.io/docs/generating.html" target="_blank" rel="external">Generating</a></p>
<h3 id="Deploy_to_remote_sites">Deploy to remote sites</h3><figure class="highlight bash"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">$ hexo deploy</span><br></pre></td></tr></table></figure>
<p>More info: <a href="http://hexo.io/docs/deployment.html" target="_blank" rel="external">Deployment</a></p>

      
    </div>
    <footer class="article-footer">
      <a data-url="http://yoursite.com/2015/05/17/hello-world/" data-id="cicclz2k4000338ffksz9bq1l" class="article-share-link">Share</a>
      
        <a href="http://yoursite.com/2015/05/17/hello-world/#disqus_thread" class="article-comment-link">Comments</a>
      
      
  <ul class="article-tag-list"><li class="article-tag-list-item"><a class="article-tag-list-link" href="/tags/Hexo/">Hexo</a></li></ul>

    </footer>
  </div>
  
</article>


  
  
</section>
        
          <aside id="sidebar">
  
    
  
    
  <div class="widget-wrap">
    <h3 class="widget-title">Tags</h3>
    <div class="widget">
      <ul class="tag-list"><li class="tag-list-item"><a class="tag-list-link" href="/tags/Hexo/">Hexo</a><span class="tag-list-count">1</span></li><li class="tag-list-item"><a class="tag-list-link" href="/tags/Machine-Learning/">Machine Learning</a><span class="tag-list-count">3</span></li></ul>
    </div>
  </div>

  
    
  <div class="widget-wrap">
    <h3 class="widget-title">Tag Cloud</h3>
    <div class="widget tagcloud">
      <a href="/tags/Hexo/" style="font-size: 10px;">Hexo</a><a href="/tags/Machine-Learning/" style="font-size: 20px;">Machine Learning</a>
    </div>
  </div>

  
    
  <div class="widget-wrap">
    <h3 class="widget-title">Archives</h3>
    <div class="widget">
      <ul class="archive-list"><li class="archive-list-item"><a class="archive-list-link" href="/archives/2015/07/">July 2015</a><span class="archive-list-count">3</span></li><li class="archive-list-item"><a class="archive-list-link" href="/archives/2015/05/">May 2015</a><span class="archive-list-count">1</span></li></ul>
    </div>
  </div>

  
    
  <div class="widget-wrap">
    <h3 class="widget-title">Recents</h3>
    <div class="widget">
      <ul>
        
          <li>
            <a href="/2015/07/20/Density-Estimation/">Density Estimation</a>
          </li>
        
          <li>
            <a href="/2015/07/18/Bayesian-Decision-Theory/">Bayesian Decision Theory</a>
          </li>
        
          <li>
            <a href="/2015/07/07/Support Vector Machine/">Support Vector Machine</a>
          </li>
        
          <li>
            <a href="/2015/05/17/hello-world/">Hello World</a>
          </li>
        
      </ul>
    </div>
  </div>

  
</aside>
        
			
      </div>
      <footer id="footer">
  
  <div class="outer">
    <div id="footer-info" class="inner">
      &copy; 2015 YYY OUC<br>
      Powered by <a href="http://hexo.io/" target="_blank">Hexo</a>
    </div>
  </div>
</footer>
    </div>
    <nav id="mobile-nav">
  
    <a href="/" class="mobile-nav-link">Home</a>
  
    <a href="/archives" class="mobile-nav-link">Archives</a>
  
</nav>
    
<script>
  var disqus_shortname = 'yyyouc';
  
  (function(){
    var dsq = document.createElement('script');
    dsq.type = 'text/javascript';
    dsq.async = true;
    dsq.src = '//' + disqus_shortname + '.disqus.com/count.js';
    (document.getElementsByTagName('head')[0] || document.getElementsByTagName('body')[0]).appendChild(dsq);
  })();
</script>


<script src="//ajax.googleapis.com/ajax/libs/jquery/2.0.3/jquery.min.js"></script>


  <link rel="stylesheet" href="/fancybox/jquery.fancybox.css" type="text/css">
  <script src="/fancybox/jquery.fancybox.pack.js" type="text/javascript"></script>


<script src="/js/script.js" type="text/javascript"></script>


<script type="text/x-mathjax-config"> 
MathJax.Hub.Config({ 
  tex2jax: {inlineMath: [['$','$'], ['\\(','\\)']]} 
}); 
</script>
<script type="text/javascript"
   src="http://cdn.mathjax.org/mathjax/latest/MathJax.js?config=TeX-AMS-MML_HTMLorMML">
</script>


	



  </div>
</body>
</html>