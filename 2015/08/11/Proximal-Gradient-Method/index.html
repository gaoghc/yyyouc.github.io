<!DOCTYPE html>
<html>
<head>
  <meta charset="utf-8">
  
  <title>Proximal Gradient Method | YYYOUC</title>
  <meta name="viewport" content="width=device-width, initial-scale=1, maximum-scale=1">
  <meta name="description" content="1. IntroductionThe proximal algorithm is widely used recently. In this work, we will talk about some basic knowldege about the proximal algorithm and some concrete proximal methods.
Much like Newton’s">
<meta property="og:type" content="article">
<meta property="og:title" content="Proximal Gradient Method">
<meta property="og:url" content="http://yoursite.com/2015/08/11/Proximal-Gradient-Method/index.html">
<meta property="og:site_name" content="YYYOUC">
<meta property="og:description" content="1. IntroductionThe proximal algorithm is widely used recently. In this work, we will talk about some basic knowldege about the proximal algorithm and some concrete proximal methods.
Much like Newton’s">
<meta name="twitter:card" content="summary">
<meta name="twitter:title" content="Proximal Gradient Method">
<meta name="twitter:description" content="1. IntroductionThe proximal algorithm is widely used recently. In this work, we will talk about some basic knowldege about the proximal algorithm and some concrete proximal methods.
Much like Newton’s">
  
    <link rel="alternative" href="/atom.xml" title="YYYOUC" type="application/atom+xml">
  
  
    <link rel="icon" href="/favicon.png">
  
  <link href="//fonts.googleapis.com/css?family=Source+Code+Pro" rel="stylesheet" type="text/css">
  <link rel="stylesheet" href="/css/style.css" type="text/css">
  

</head>
<body>
  <div id="container">
    <div id="wrap">
      <header id="header">
  <div id="banner"></div>
  <div id="header-outer" class="outer">
    <div id="header-title" class="inner">
      <h1 id="logo-wrap">
        <a href="/" id="logo">YYYOUC</a>
      </h1>
      
    </div>
    <div id="header-inner" class="inner">
      <nav id="main-nav">
        <a id="main-nav-toggle" class="nav-icon"></a>
        
          <a class="main-nav-link" href="/">Home</a>
        
          <a class="main-nav-link" href="/archives">Archives</a>
        
      </nav>
      <nav id="sub-nav">
        
          <a id="nav-rss-link" class="nav-icon" href="/atom.xml" title="RSS Feed"></a>
        
        <a id="nav-search-btn" class="nav-icon" title="Search"></a>
      </nav>
      <div id="search-form-wrap">
        <form action="//google.com/search" method="get" accept-charset="UTF-8" class="search-form"><input type="search" name="q" results="0" class="search-form-input" placeholder="Search"><button type="submit" class="search-form-submit">&#xF002;</button><input type="hidden" name="q" value="site:http://yoursite.com"></form>
      </div>
    </div>
  </div>
</header>
      <div class="outer">
        <section id="main"><article id="post-Proximal-Gradient-Method" class="article article-type-post" itemscope itemprop="blogPost">
  <div class="article-meta">
    <a href="/2015/08/11/Proximal-Gradient-Method/" class="article-date">
  <time datetime="2015-08-12T01:22:29.000Z" itemprop="datePublished">2015-08-11</time>
</a>
    
  </div>
  <div class="article-inner">
    
    
      <header class="article-header">
        
  
    <h1 class="article-title" itemprop="name">
      Proximal Gradient Method
    </h1>
  

      </header>
    
    <div class="article-entry" itemprop="articleBody">
      
        <h2 id="1-_Introduction">1. Introduction</h2><p>The proximal algorithm is widely used recently. In this work, we will talk about some basic knowldege about the proximal algorithm and some concrete proximal methods.</p>
<p>Much like Newton’s method is a standard tool for solving unconstrained smooth minimization problems of modest size, proximal algorithms can be viewed as an analogous tool for <strong>nonsmooth, constrained, large-scale, or distributed</strong> versions of these problems.</p>
<p>In fact, the proximal algorithm is to solve the <strong>upper bound</strong> of the original problem to get the solution. We will talk about it in those concrete proximal algorithms.</p>
<h2 id="2-_Proximal_Mapping">2. Proximal Mapping</h2><p>In proximal algorithm, the base operation is evaluating the <strong>proximal operator</strong> of a function, which involves solving a small convex optimization problem. These subproblems can be solved with standard methods. More important, they often have closed-form solutions or can be solved very quickly with simple specialized methods.</p>
<h3 id="2-1_Definition">2.1 Definition</h3><p>The <strong>proximal operator</strong> $prox_f$ of function $f$ is defined as:<br>$$<br>prox_f(x) = \arg\min_{u}(f(u)+\frac{1}{2}||u-x||_2^2)<br>$$<br>If $f$ is closed and convex, $prox_f(x)$ <strong>exists</strong> and is <strong>unique</strong> for all $x$. Because the second derivative of the righthand side is $$\nabla^2 f(u) + I\geq I$$, then it is a strongly convex function. Therefore, it has unique optimal point.</p>
<p>In machine learning field, we often encounter the proximal operator of the scaled function $\lambda f$ as follows.<br>$$<br>prox_{\lambda f}(x) = \arg\min_{u}(f(u)+\frac{1}{2\lambda}||u-x||_2^2)<br>$$</p>
<h3 id="2-2_Examples">2.2 Examples</h3><p>(1). $f(x)=0$: $prox_f(x) = x$ </p>
<p>(2). $f(x)=I_C(x)$ (indictor function of C): $prox_h$ is the projection on C. $prox_h(x)=\arg\min_{u\in C}||u-x||_2^2=P_C(x)$</p>
<p>(3). $f(x)=||x||_1$: $prox_h$ is the ‘soft-threshold’ (shrinkage) operation<br>$$ prox_h(x)_i=\left\{<br>\begin{array}{lcl}<br>x_i-1       &amp;      &amp; {x_i\geq 1}\\<br>0     &amp;      &amp; {|x_i| \leq 1}\\<br>x_i+1     &amp;      &amp; {x_i\leq -1}\\<br>\end{array} \right. $$</p>
<h2 id="3-_Proximal_Gradient_Method">3. Proximal Gradient Method</h2><p>In machine learning application, we often encounter the following objective function:<br>$$<br>\min f(x)+\lambda h(x)<br>$$ where $f(x)$ is convex and differentiable, usually it is a loss function. $h(x)$ is convex with inexpensive prox-perator,usually it is a regularizer. For example, the hinge regression problem<br>$$\min_{W} ||Y-W^TX||_F^2+||W||_1$$ where $f(W)=||Y-W^TX||_F^2$, $h(W) = ||W||_1$.</p>
<h3 id="3-1_Key_Idea">3.1 Key Idea</h3><p><strong>The key idea of the proximal gradient method is to minimize the upper bound of the origial objective</strong>.<br>The general fomulation of proximal gradient method is as follows.<br>$$<br>x_{t+1} = prox_{s_{k+1}h}(x_{t}-s_{k+1}\nabla f(x_t))<br>$$where $s_{k+1}&gt;0$ is the step size. </p>
<p>However, what’s the meaning of this iterative step? Assume $f(x)$ is Lipschitz continous, then<br>$$f(x)\leq f(x_t)+ \nabla f(x_t)^T(x-x_t) + \frac{L}{2}||x-x_t||_2^2 \triangleq g(x)$$ The upper bound $g(x)=\frac{L}{2}||x-a||_2^2+c$ where $a=x_t-\frac{1}{L}\nabla f(x_t)$ and $c=f(x_t)-\frac{1}{2L}||\nabla f(x_t)||_2^2$. Therefore, using the upper bound $g(x)$, we can solve $\min f(x)+\lambda h(x)$  by<br>$$<br>\min \frac{L}{2}||x-a||_2^2 +\lambda h(x)<br>$$where $a=x_t-\frac{1}{L}\nabla f(x_t)$. Usually, the new objective is easier to solve than the original one.<br>Therefore,<br>$$<br>x_{t+1}=\arg\min_{x}(\frac{L}{2}||x-a||_2^2 +\lambda h(x)) \<br>= \arg\min_{x}(||x-a||_2^2 +\frac{2\lambda}{L} h(x)) \<br>= prox_{\frac{2\lambda}{L} h}(a)<br>= prox_{\frac{2\lambda}{L} h}(x_t-\frac{1}{L}\nabla f(x_t))<br>$$<br>All in all, we can conclude that the proximal gradient method is to solve the upper bound of the original problem.</p>
<h3 id="3-2_Convergence">3.2 Convergence</h3><p>The objective function is as follows:<br>$$<br>\min_{x} f(x)=g(x)+h(x)<br>$$ The iterative step of proximal gradient method is<br>$$<br>x_{k} = prox_{t_kh}(x_{k-1}-t\nabla g(x_{k-1}))<br>$$ We can reformulate the iterative step as follows:<br>$$x_{k} = prox_{t_kh}(x_{k-1}-t\nabla g(x_{k-1})) \<br>=x_{k-1}-tG_t(x_{k-1})<br>$$ where $G_t(x_{k-1}) = \frac{1}{t}(x_{k-1}-prox_{th}(x_{k-1}-t\nabla g(x_{k-1})))$. We can view $G_t(x)$ as the <strong>‘step’</strong> in the proximal gradient update.</p>
<ul>
<li>Note that $G_t(x)$ is not the gradient or subgradient of $f=g+h$</li>
<li>From the subgradient definition of prox-operator, we have<br>$$<br>G_t(x) \in \nabla g(x) + \partial h(x-tG_t(x))<br>$$Because $u=prox_h(x)$ means $\partial h(u) + u -x = 0$, then $x-u\in \partial h(u)$</li>
</ul>
<h4 id="3-2-1_Assumptions">3.2.1 Assumptions</h4><ul>
<li>$g(x)$ is convex, and $\nabla g(x)$ is Lipschitz continous with constant L:<br>$$||\nabla g(x) - \nabla g(y)||_2\leq ||x-y||_2$$</li>
<li>$h(x)$ is closed and convex, then $porx_{th}$ is well defined.</li>
</ul>
<h4 id="3-2-2_Convergence_of_fixed_step_length">3.2.2 Convergence of fixed step length</h4><p>Due to the convexity and the Lipschitz continous gradient of $g$, we have<br>$$<br>g(y)\leq g(x)+\nabla g(x)^T(y-x) + \frac{L}{2}||y-x||_2^2<br>$$ Let $y=x-tG_t(x)$, we have<br>$$<br>g(x-tG_t(x))\leq g(x) - t\nabla g(x)^TG_t(x)+\frac{t^2L}{2}||G_t(x)||_2^2<br>$$ If $0\leq t \leq 1/L$, then<br>$$<br>g(x-tG_t(x))\leq g(x) - t\nabla g(x)^TG_t(x)+\frac{t}{2}||G_t(x)||_2^2<br>$$<br>Then, for the entire objective function, we have the following inequality for all $z$<br>$$<br>f(x-tG_t(x))\leq f(z)+G_t(x)^T(x-z)-\frac{t}{2}||G_t(x)||_2^2<br>$$ Because<br>$$<br>f(x-tG_t(x))\leq g(x) - t\nabla g(x)^TG_t(x)+\frac{t}{2}||G_t(x)||_2^2 + h(x-tG_t(x)) \\<br>\leq g(z)+\nabla g(x)^T(x-z)-t\nabla g(x)^TG_t(x)+\frac{t}{2}||G_t(x)||_2^2 +h(z)+v^T(x-z-tG_t(x)) \\<br>= g(z) + h(z) + G_t(x)^T(x-z)-\frac{t}{2}||G_t(x)||_2^2<br>$$ where $v=G_t(x)-\nabla g(x) \in \partial h(x-tG_t(x))$.</p>
<p>Let $x_{k+1}=x_k-tG_t(x)$, $z=x_k$, we have<br>$$<br>f(x_{k+1})\leq f(x_{k}) - \frac{t}{2} ||G_t(x_k)||_2^2<br>$$<br>And if we let $z=x_{opt}$, we have<br>$$<br>f(x_{k+1})-f(x_{opt})\leq G_t(x_k)^T(x_k-x_{opt})-\frac{t}{2}||G_t(x_k)||_2^2 \\<br>= \frac{1}{2t}(||x_k-x_{opt}||_2^2-||x_k-x_{opt}-tG_t(x_k)||_2^2) \\<br>=\frac{1}{2t}(||x_k-x_{opt}||_2^2-||x_{k+1}-x_{opt}||_2^2)<br>$$<br>Therefore,<br>$$<br>\sum_{i=1}^k(f(x_i)-f_{opt}) \leq \frac{1}{2t}\sum_{i=1}^k(||x_{i-1}-x_{opt}||_2^2-||x_i-x_{opt}||_2^2) \\<br>= \frac{1}{2t} (||x_{0}-x_{opt}||_2^2-||x_k-x_{opt}||_2^2) \\<br>\leq \frac{1}{2t}||x_{0}-x_{opt}||_2^2<br>$$<br>Since $f$ is nonincreasing, we have<br>$$<br>f(x_k)-f_{opt} \leq \frac{1}{k}\sum_{i=1}^k(f(x_i)-f_{opt}) \leq \frac{1}{2kt}||x_0 - x_{opt}||_2^2<br>$$<br>Hence, it reaches $f(x_k)-f_{opt} \leq \epsilon$ after $O(1/\epsilon)$ iterations.</p>
<h4 id="3-2-3_Convergence_of_line_search_method">3.2.3 Convergence of line search method</h4><p>For the line search method, we have<br>$$<br>f(x_{k+1})-f(x_{opt})\leq \frac{1}{2t_i}(||x_k-x_{opt}||_2^2-||x_{k+1}-x_{opt}||_2^2) \<br>\leq \frac{1}{2t_{min}}(||x_k-x_{opt}||_2^2-||x_{k+1}-x_{opt}||_2^2)<br>$$ Then<br>$$<br>\sum_{i=1}^k(f(x_i)-f_{opt}) \leq \frac{1}{2t_{min}}||x_0 - x_{opt}||_2^2<br>$$<br>Similarly,$f$ is nonincreasing, we have<br>$$<br>f(x_k)-f_{opt}  \leq \frac{1}{2kt_{min}}||x_0 - x_{opt}||_2^2<br>$$<br>Hence, it also reaches $f(x_k)-f_{opt} \leq \epsilon$ after $O(1/\epsilon)$ iterations.</p>

      
    </div>
    <footer class="article-footer">
      <a data-url="http://yoursite.com/2015/08/11/Proximal-Gradient-Method/" data-id="cid9npszd000bewff007o1ch4" class="article-share-link">Share</a>
      
        <a href="http://yoursite.com/2015/08/11/Proximal-Gradient-Method/#disqus_thread" class="article-comment-link">Comments</a>
      
      
  <ul class="article-tag-list"><li class="article-tag-list-item"><a class="article-tag-list-link" href="/tags/Optimization-Method/">Optimization Method</a></li></ul>

    </footer>
  </div>
  
    
<nav id="article-nav">
  
  
    <a href="/2015/08/01/Linear-Disciminant-Analysis/" id="article-nav-older" class="article-nav-link-wrap">
      <strong class="article-nav-caption">Older</strong>
      <div class="article-nav-title">Linear Disciminant Analysis</div>
    </a>
  
</nav>

  
</article>


<section id="comments">
	<div id="disqus_thread"></div>
	<script type="text/javascript">
		/* * * CONFIGURATION VARIABLES * * */
		var disqus_shortname = 'yyyoucgithubio';
		
		/* * * DON'T EDIT BELOW THIS LINE * * */
		(function() {
			var dsq = document.createElement('script'); dsq.type = 'text/javascript'; dsq.async = true;
			dsq.src = '//' + disqus_shortname + '.disqus.com/embed.js';
			(document.getElementsByTagName('head')[0] || document.getElementsByTagName('body')[0]).appendChild(dsq);
		})();
	</script>
	<noscript>Please enable JavaScript to view the <a href="https://disqus.com/?ref_noscript" rel="nofollow">comments powered by Disqus.</a></noscript>
</section>
</section>
        
          <aside id="sidebar">
  
    
  
    
  <div class="widget-wrap">
    <h3 class="widget-title">Tags</h3>
    <div class="widget">
      <ul class="tag-list"><li class="tag-list-item"><a class="tag-list-link" href="/tags/Hexo/">Hexo</a><span class="tag-list-count">1</span></li><li class="tag-list-item"><a class="tag-list-link" href="/tags/Machine-Learning/">Machine Learning</a><span class="tag-list-count">4</span></li><li class="tag-list-item"><a class="tag-list-link" href="/tags/Optimization-Method/">Optimization Method</a><span class="tag-list-count">4</span></li></ul>
    </div>
  </div>

  
    
  <div class="widget-wrap">
    <h3 class="widget-title">Tag Cloud</h3>
    <div class="widget tagcloud">
      <a href="/tags/Hexo/" style="font-size: 10px;">Hexo</a><a href="/tags/Machine-Learning/" style="font-size: 20px;">Machine Learning</a><a href="/tags/Optimization-Method/" style="font-size: 20px;">Optimization Method</a>
    </div>
  </div>

  
    
  <div class="widget-wrap">
    <h3 class="widget-title">Archives</h3>
    <div class="widget">
      <ul class="archive-list"><li class="archive-list-item"><a class="archive-list-link" href="/archives/2015/08/">August 2015</a><span class="archive-list-count">2</span></li><li class="archive-list-item"><a class="archive-list-link" href="/archives/2015/07/">July 2015</a><span class="archive-list-count">6</span></li><li class="archive-list-item"><a class="archive-list-link" href="/archives/2015/05/">May 2015</a><span class="archive-list-count">1</span></li></ul>
    </div>
  </div>

  
    
  <div class="widget-wrap">
    <h3 class="widget-title">Recents</h3>
    <div class="widget">
      <ul>
        
          <li>
            <a href="/2015/08/11/Proximal-Gradient-Method/">Proximal Gradient Method</a>
          </li>
        
          <li>
            <a href="/2015/08/01/Linear-Disciminant-Analysis/">Linear Disciminant Analysis</a>
          </li>
        
          <li>
            <a href="/2015/07/29/Subgradient-Method/">Subgradient Method</a>
          </li>
        
          <li>
            <a href="/2015/07/24/Gradient-Descent-Method/">Gradient Descent Method</a>
          </li>
        
          <li>
            <a href="/2015/07/21/Convex-Function/">Convex Function</a>
          </li>
        
      </ul>
    </div>
  </div>

  
</aside>
        
		
<section id="comment">	
	<div id="disqus_thread"></div>
	<script type="text/javascript">
		/* * * CONFIGURATION VARIABLES * * */
		var disqus_shortname = 'yyyouc';
		
		/* * * DON'T EDIT BELOW THIS LINE * * */
		(function() {
			var dsq = document.createElement('script'); dsq.type = 'text/javascript'; dsq.async = true;
			dsq.src = '//' + disqus_shortname + '.disqus.com/embed.js';
			(document.getElementsByTagName('head')[0] || document.getElementsByTagName('body')[0]).appendChild(dsq);
		})();
	</script>
	<noscript>Please enable JavaScript to view the <a href="https://disqus.com/?ref_noscript" rel="nofollow">comments powered by Disqus.</a></noscript>
<a href="http://disqus.com" class="dsq-brlink">comments powered by <span class="logo-disqus">Disqus</span></a>
</section>
	
      </div>
      <footer id="footer">
  
  <div class="outer">
    <div id="footer-info" class="inner">
      &copy; 2015 YYY OUC<br>
      Powered by <a href="http://hexo.io/" target="_blank">Hexo</a>
    </div>
  </div>
</footer>
    </div>
    <nav id="mobile-nav">
  
    <a href="/" class="mobile-nav-link">Home</a>
  
    <a href="/archives" class="mobile-nav-link">Archives</a>
  
</nav>
    
<script>
  var disqus_shortname = 'yyyouc';
  
  var disqus_url = 'http://yoursite.com/2015/08/11/Proximal-Gradient-Method/';
  
  (function(){
    var dsq = document.createElement('script');
    dsq.type = 'text/javascript';
    dsq.async = true;
    dsq.src = '//' + disqus_shortname + '.disqus.com/embed.js';
    (document.getElementsByTagName('head')[0] || document.getElementsByTagName('body')[0]).appendChild(dsq);
  })();
</script>


<script src="//ajax.googleapis.com/ajax/libs/jquery/2.0.3/jquery.min.js"></script>


  <link rel="stylesheet" href="/fancybox/jquery.fancybox.css" type="text/css">
  <script src="/fancybox/jquery.fancybox.pack.js" type="text/javascript"></script>


<script src="/js/script.js" type="text/javascript"></script>


<script type="text/x-mathjax-config"> 
MathJax.Hub.Config({ 
  tex2jax: {inlineMath: [['$','$'], ['\\(','\\)']]} 
}); 
</script>
<script type="text/javascript"
   src="http://cdn.mathjax.org/mathjax/latest/MathJax.js?config=TeX-AMS-MML_HTMLorMML">
</script>


	


<!-- mathjax config similar to math.stackexchange -->

<script type="text/x-mathjax-config">
  MathJax.Hub.Config({
    tex2jax: {
      inlineMath: [ ['$','$'], ["\\(","\\)"] ],
      processEscapes: true
    }
  });
</script>

<script type="text/x-mathjax-config">
    MathJax.Hub.Config({
      tex2jax: {
        skipTags: ['script', 'noscript', 'style', 'textarea', 'pre', 'code']
      }
    });
</script>

<script type="text/x-mathjax-config">
    MathJax.Hub.Queue(function() {
        var all = MathJax.Hub.getAllJax(), i;
        for(i=0; i < all.length; i += 1) {
            all[i].SourceElement().parentNode.className += ' has-jax';
        }
    });
</script>

<script type="text/javascript" src="http://cdn.mathjax.org/mathjax/latest/MathJax.js?config=TeX-AMS-MML_HTMLorMML">
</script>


  </div>
</body>
</html>