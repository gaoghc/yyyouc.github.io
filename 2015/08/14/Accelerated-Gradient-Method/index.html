<!DOCTYPE html>
<html>
<head>
  <meta charset="utf-8">
  
  <title>Accelerated Gradient Method | YYYOUC</title>
  <meta name="viewport" content="width=device-width, initial-scale=1, maximum-scale=1">
  <meta name="description" content="1. IntroductionWe have talked about three gradient-based method (or first order method) in the previous work, that is gradient descent method, subgradient method and proximal gradient method. Their co">
<meta property="og:type" content="article">
<meta property="og:title" content="Accelerated Gradient Method">
<meta property="og:url" content="http://yoursite.com/2015/08/14/Accelerated-Gradient-Method/index.html">
<meta property="og:site_name" content="YYYOUC">
<meta property="og:description" content="1. IntroductionWe have talked about three gradient-based method (or first order method) in the previous work, that is gradient descent method, subgradient method and proximal gradient method. Their co">
<meta name="twitter:card" content="summary">
<meta name="twitter:title" content="Accelerated Gradient Method">
<meta name="twitter:description" content="1. IntroductionWe have talked about three gradient-based method (or first order method) in the previous work, that is gradient descent method, subgradient method and proximal gradient method. Their co">
  
    <link rel="alternative" href="/atom.xml" title="YYYOUC" type="application/atom+xml">
  
  
    <link rel="icon" href="/favicon.png">
  
  <link href="//fonts.googleapis.com/css?family=Source+Code+Pro" rel="stylesheet" type="text/css">
  <link rel="stylesheet" href="/css/style.css" type="text/css">
  

</head>
<body>
  <div id="container">
    <div id="wrap">
      <header id="header">
  <div id="banner"></div>
  <div id="header-outer" class="outer">
    <div id="header-title" class="inner">
      <h1 id="logo-wrap">
        <a href="/" id="logo">YYYOUC</a>
      </h1>
      
    </div>
    <div id="header-inner" class="inner">
      <nav id="main-nav">
        <a id="main-nav-toggle" class="nav-icon"></a>
        
          <a class="main-nav-link" href="/">Home</a>
        
          <a class="main-nav-link" href="/archives">Archives</a>
        
      </nav>
      <nav id="sub-nav">
        
          <a id="nav-rss-link" class="nav-icon" href="/atom.xml" title="RSS Feed"></a>
        
        <a id="nav-search-btn" class="nav-icon" title="Search"></a>
      </nav>
      <div id="search-form-wrap">
        <form action="//google.com/search" method="get" accept-charset="UTF-8" class="search-form"><input type="search" name="q" results="0" class="search-form-input" placeholder="Search"><button type="submit" class="search-form-submit">&#xF002;</button><input type="hidden" name="q" value="site:http://yoursite.com"></form>
      </div>
    </div>
  </div>
</header>
      <div class="outer">
        <section id="main"><article id="post-Accelerated-Gradient-Method" class="article article-type-post" itemscope itemprop="blogPost">
  <div class="article-meta">
    <a href="/2015/08/14/Accelerated-Gradient-Method/" class="article-date">
  <time datetime="2015-08-14T19:55:55.000Z" itemprop="datePublished">2015-08-14</time>
</a>
    
  </div>
  <div class="article-inner">
    
    
      <header class="article-header">
        
  
    <h1 class="article-title" itemprop="name">
      Accelerated Gradient Method
    </h1>
  

      </header>
    
    <div class="article-entry" itemprop="articleBody">
      
        <h2 id="1-_Introduction">1. Introduction</h2><p>We have talked about three gradient-based method (or first order method) in the previous work, that is <strong>gradient descent method</strong>, <strong>subgradient method</strong> and <strong>proximal gradient method</strong>. Their convergence rate is $O(1/k)$, $O(1/\sqrt(k)$ and $O(1/k)$ respectively. Today we will talk about the accelerated gradient method whose convergence rate is $O(1/k^2)$.</p>
<p>In the following, we will talk about Nesterov’s three accelerated methods corresponding his three papers:</p>
<ul>
<li>Y. Nesterov (1983), A method for solving a convex programming problem with convergence rate $O(1/k^2)$</li>
<li>Y. Nesterov (1988) On an approach to the construction of optimal methods of minimization of smooth convex functions</li>
<li>Y. Nesterov (2005), Smooth minimization of non-smooth functions<br>In addition, we will talk about <strong>FISTA</strong> method</li>
</ul>
<p>There is an assumption about $f$:</p>
<ul>
<li>$f$ is a convex function with an $L$-Lipshitz continuous derivative.</li>
</ul>
<h2 id="2-_Nesterov’s_First_Method_(1983)">2. Nesterov’s First Method (1983)</h2><p>Choose $x_0$ and set $x_0=x_{-1}$. For all $k\geq 1$, repeat the steps</p>
<ul>
<li>$y = x_{k-1} + \frac{k-2}{k+1}(x_{k-1} - x_{k-2})$</li>
<li>$x_k = y-t_k\nabla f(y)$</li>
</ul>
<p>where $t_k$ is the step length. </p>
<p>Note that</p>
<ul>
<li>First iteration (k=1) is a gradient step at $y=x_0$</li>
<li>Next iterations are the gradient steps at extrapolated points $y$</li>
<li>$\frac{k-2}{k+1}(x_{k-1} - x_{k-2})$ can be viewed as the <strong>momentum term</strong>.</li>
</ul>
<p>Define $\theta_k = \frac{2}{k+1}$ and introduce an intermediate variable $v_k$. We can reformulate the above iterative steps as follows:<br>Choose $x_0=v_0$, for all $k\geq 1$, repeat the steps:</p>
<ul>
<li>$y = (1-\theta_k) x_{k-1} + \theta_k v_{k-1}$</li>
<li>$x_{k} = y - t_k\nabla f(y)$</li>
<li>$v_{k} = x_{k-1}+\frac{1}{\theta_k}(x_k-x_{k-1})$</li>
</ul>
<p>This expression makes convergence analysis easier.</p>
<h2 id="3-_Nesterov’s_Second_Method_(1988)">3. Nesterov’s Second Method (1988)</h2><p>Choose $x_0=v_0$, for all $k\geq 1$, repeat the steps</p>
<ul>
<li>$y = (1-\theta_k)x_{k-1}+\theta_k v_{k-1}$</li>
<li>$v_k =v_{k-1} - \frac{t_k}{\theta_k}\nabla g(y)$</li>
<li>$x_k = (1-\theta_k)x_{k-1} + \theta_k v_{k}$</li>
</ul>
<p>where $t_k$ is the step size and $\theta_k=\frac{2}{k+1}$</p>
<h2 id="4-_Nesterov’s_Third_Method_(2005)">4. Nesterov’s Third Method (2005)</h2><p>abc</p>
<h2 id="5-_Another_Formulation_(Beck_and_Teboulle_2009)">5. Another Formulation (Beck and Teboulle 2009)</h2><p>The general iterative step of the first method is as follows:<br>Choose $x_1$, set $y_1=x_1$, $\theta_1=1$, repeates:</p>
<ul>
<li>$x_{k+1}=y_{k} - t_k \nabla f(y_{k})$</li>
<li>$\theta_{k+1} = \frac{1+\sqrt{1+4\theta_{k}^2}}{2}$</li>
<li>$y_{k+1} = x_{k+1}+\frac{\theta_{k} - 1}{\theta_{k+1}}(x_{k+1}-x_{k})$</li>
</ul>
<p>where $t_k$ is the step size. Note that the three properties of $\theta_{k}$:</p>
<ul>
<li>$\theta_{k}$ is positive and increasing</li>
<li>$\theta_{k+1} \geq \theta_{k} + 1/2$</li>
<li>$\lim_{t-&gt;inf}\frac{\theta_{k} - 1}{\theta_{k+1}}=1$ </li>
</ul>
<h2 id="6-_Convergence_Rate">6. Convergence Rate</h2><p>If $f$ is a convex function with an $L$-Lipshitz continuous derivative, then the above method satisfies the following:</p>

      
    </div>
    <footer class="article-footer">
      <a data-url="http://yoursite.com/2015/08/14/Accelerated-Gradient-Method/" data-id="cidcgcnxt0000f4ffc03xblti" class="article-share-link">Share</a>
      
        <a href="http://yoursite.com/2015/08/14/Accelerated-Gradient-Method/#disqus_thread" class="article-comment-link">Comments</a>
      
      
  <ul class="article-tag-list"><li class="article-tag-list-item"><a class="article-tag-list-link" href="/tags/Optimization-Method/">Optimization Method</a></li></ul>

    </footer>
  </div>
  
    
<nav id="article-nav">
  
  
    <a href="/2015/08/11/Proximal-Gradient-Method/" id="article-nav-older" class="article-nav-link-wrap">
      <strong class="article-nav-caption">Older</strong>
      <div class="article-nav-title">Proximal Gradient Method</div>
    </a>
  
</nav>

  
</article>


<section id="comments">
	<div id="disqus_thread"></div>
	<script type="text/javascript">
		/* * * CONFIGURATION VARIABLES * * */
		var disqus_shortname = 'yyyoucgithubio';
		
		/* * * DON'T EDIT BELOW THIS LINE * * */
		(function() {
			var dsq = document.createElement('script'); dsq.type = 'text/javascript'; dsq.async = true;
			dsq.src = '//' + disqus_shortname + '.disqus.com/embed.js';
			(document.getElementsByTagName('head')[0] || document.getElementsByTagName('body')[0]).appendChild(dsq);
		})();
	</script>
	<noscript>Please enable JavaScript to view the <a href="https://disqus.com/?ref_noscript" rel="nofollow">comments powered by Disqus.</a></noscript>
</section>
</section>
        
          <aside id="sidebar">
  
    
  
    
  <div class="widget-wrap">
    <h3 class="widget-title">Tags</h3>
    <div class="widget">
      <ul class="tag-list"><li class="tag-list-item"><a class="tag-list-link" href="/tags/Hexo/">Hexo</a><span class="tag-list-count">1</span></li><li class="tag-list-item"><a class="tag-list-link" href="/tags/Machine-Learning/">Machine Learning</a><span class="tag-list-count">4</span></li><li class="tag-list-item"><a class="tag-list-link" href="/tags/Optimization-Method/">Optimization Method</a><span class="tag-list-count">5</span></li></ul>
    </div>
  </div>

  
    
  <div class="widget-wrap">
    <h3 class="widget-title">Tag Cloud</h3>
    <div class="widget tagcloud">
      <a href="/tags/Hexo/" style="font-size: 10px;">Hexo</a><a href="/tags/Machine-Learning/" style="font-size: 15px;">Machine Learning</a><a href="/tags/Optimization-Method/" style="font-size: 20px;">Optimization Method</a>
    </div>
  </div>

  
    
  <div class="widget-wrap">
    <h3 class="widget-title">Archives</h3>
    <div class="widget">
      <ul class="archive-list"><li class="archive-list-item"><a class="archive-list-link" href="/archives/2015/08/">August 2015</a><span class="archive-list-count">3</span></li><li class="archive-list-item"><a class="archive-list-link" href="/archives/2015/07/">July 2015</a><span class="archive-list-count">6</span></li><li class="archive-list-item"><a class="archive-list-link" href="/archives/2015/05/">May 2015</a><span class="archive-list-count">1</span></li></ul>
    </div>
  </div>

  
    
  <div class="widget-wrap">
    <h3 class="widget-title">Recents</h3>
    <div class="widget">
      <ul>
        
          <li>
            <a href="/2015/08/14/Accelerated-Gradient-Method/">Accelerated Gradient Method</a>
          </li>
        
          <li>
            <a href="/2015/08/11/Proximal-Gradient-Method/">Proximal Gradient Method</a>
          </li>
        
          <li>
            <a href="/2015/08/01/Linear-Disciminant-Analysis/">Linear Disciminant Analysis</a>
          </li>
        
          <li>
            <a href="/2015/07/29/Subgradient-Method/">Subgradient Method</a>
          </li>
        
          <li>
            <a href="/2015/07/24/Gradient-Descent-Method/">Gradient Descent Method</a>
          </li>
        
      </ul>
    </div>
  </div>

  
</aside>
        
		
<section id="comment">	
	<div id="disqus_thread"></div>
	<script type="text/javascript">
		/* * * CONFIGURATION VARIABLES * * */
		var disqus_shortname = 'yyyouc';
		
		/* * * DON'T EDIT BELOW THIS LINE * * */
		(function() {
			var dsq = document.createElement('script'); dsq.type = 'text/javascript'; dsq.async = true;
			dsq.src = '//' + disqus_shortname + '.disqus.com/embed.js';
			(document.getElementsByTagName('head')[0] || document.getElementsByTagName('body')[0]).appendChild(dsq);
		})();
	</script>
	<noscript>Please enable JavaScript to view the <a href="https://disqus.com/?ref_noscript" rel="nofollow">comments powered by Disqus.</a></noscript>
<a href="http://disqus.com" class="dsq-brlink">comments powered by <span class="logo-disqus">Disqus</span></a>
</section>
	
      </div>
      <footer id="footer">
  
  <div class="outer">
    <div id="footer-info" class="inner">
      &copy; 2015 YYY OUC<br>
      Powered by <a href="http://hexo.io/" target="_blank">Hexo</a>
    </div>
  </div>
</footer>
    </div>
    <nav id="mobile-nav">
  
    <a href="/" class="mobile-nav-link">Home</a>
  
    <a href="/archives" class="mobile-nav-link">Archives</a>
  
</nav>
    
<script>
  var disqus_shortname = 'yyyouc';
  
  var disqus_url = 'http://yoursite.com/2015/08/14/Accelerated-Gradient-Method/';
  
  (function(){
    var dsq = document.createElement('script');
    dsq.type = 'text/javascript';
    dsq.async = true;
    dsq.src = '//' + disqus_shortname + '.disqus.com/embed.js';
    (document.getElementsByTagName('head')[0] || document.getElementsByTagName('body')[0]).appendChild(dsq);
  })();
</script>


<script src="//ajax.googleapis.com/ajax/libs/jquery/2.0.3/jquery.min.js"></script>


  <link rel="stylesheet" href="/fancybox/jquery.fancybox.css" type="text/css">
  <script src="/fancybox/jquery.fancybox.pack.js" type="text/javascript"></script>


<script src="/js/script.js" type="text/javascript"></script>


<script type="text/x-mathjax-config"> 
MathJax.Hub.Config({ 
  tex2jax: {inlineMath: [['$','$'], ['\\(','\\)']]} 
}); 
</script>
<script type="text/javascript"
   src="http://cdn.mathjax.org/mathjax/latest/MathJax.js?config=TeX-AMS-MML_HTMLorMML">
</script>


	


<!-- mathjax config similar to math.stackexchange -->

<script type="text/x-mathjax-config">
  MathJax.Hub.Config({
    tex2jax: {
      inlineMath: [ ['$','$'], ["\\(","\\)"] ],
      processEscapes: true
    }
  });
</script>

<script type="text/x-mathjax-config">
    MathJax.Hub.Config({
      tex2jax: {
        skipTags: ['script', 'noscript', 'style', 'textarea', 'pre', 'code']
      }
    });
</script>

<script type="text/x-mathjax-config">
    MathJax.Hub.Queue(function() {
        var all = MathJax.Hub.getAllJax(), i;
        for(i=0; i < all.length; i += 1) {
            all[i].SourceElement().parentNode.className += ' has-jax';
        }
    });
</script>

<script type="text/javascript" src="http://cdn.mathjax.org/mathjax/latest/MathJax.js?config=TeX-AMS-MML_HTMLorMML">
</script>


  </div>
</body>
</html>