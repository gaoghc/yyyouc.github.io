<!DOCTYPE html>
<html>
<head>
  <meta charset="utf-8">
  
  <title>Linear Disciminant Analysis | YYYOUC</title>
  <meta name="viewport" content="width=device-width, initial-scale=1, maximum-scale=1">
  <meta name="description" content="IntroductionIn machine learning and data mining applications, the data points usually lie in a high dimension space. Therefore, it’s better to perform dimension reduction to these data points. There a">
<meta property="og:type" content="article">
<meta property="og:title" content="Linear Disciminant Analysis">
<meta property="og:url" content="http://yoursite.com/2015/08/01/Linear-Disciminant-Analysis/index.html">
<meta property="og:site_name" content="YYYOUC">
<meta property="og:description" content="IntroductionIn machine learning and data mining applications, the data points usually lie in a high dimension space. Therefore, it’s better to perform dimension reduction to these data points. There a">
<meta name="twitter:card" content="summary">
<meta name="twitter:title" content="Linear Disciminant Analysis">
<meta name="twitter:description" content="IntroductionIn machine learning and data mining applications, the data points usually lie in a high dimension space. Therefore, it’s better to perform dimension reduction to these data points. There a">
  
    <link rel="alternative" href="/atom.xml" title="YYYOUC" type="application/atom+xml">
  
  
    <link rel="icon" href="/favicon.png">
  
  <link href="//fonts.googleapis.com/css?family=Source+Code+Pro" rel="stylesheet" type="text/css">
  <link rel="stylesheet" href="/css/style.css" type="text/css">
  

</head>
<body>
  <div id="container">
    <div id="wrap">
      <header id="header">
  <div id="banner"></div>
  <div id="header-outer" class="outer">
    <div id="header-title" class="inner">
      <h1 id="logo-wrap">
        <a href="/" id="logo">YYYOUC</a>
      </h1>
      
    </div>
    <div id="header-inner" class="inner">
      <nav id="main-nav">
        <a id="main-nav-toggle" class="nav-icon"></a>
        
          <a class="main-nav-link" href="/">Home</a>
        
          <a class="main-nav-link" href="/archives">Archives</a>
        
      </nav>
      <nav id="sub-nav">
        
          <a id="nav-rss-link" class="nav-icon" href="/atom.xml" title="RSS Feed"></a>
        
        <a id="nav-search-btn" class="nav-icon" title="Search"></a>
      </nav>
      <div id="search-form-wrap">
        <form action="//google.com/search" method="get" accept-charset="UTF-8" class="search-form"><input type="search" name="q" results="0" class="search-form-input" placeholder="Search"><button type="submit" class="search-form-submit">&#xF002;</button><input type="hidden" name="q" value="site:http://yoursite.com"></form>
      </div>
    </div>
  </div>
</header>
      <div class="outer">
        <section id="main"><article id="post-Linear-Disciminant-Analysis" class="article article-type-post" itemscope itemprop="blogPost">
  <div class="article-meta">
    <a href="/2015/08/01/Linear-Disciminant-Analysis/" class="article-date">
  <time datetime="2015-08-01T21:23:31.000Z" itemprop="datePublished">2015-08-01</time>
</a>
    
  </div>
  <div class="article-inner">
    
    
      <header class="article-header">
        
  
    <h1 class="article-title" itemprop="name">
      Linear Disciminant Analysis
    </h1>
  

      </header>
    
    <div class="article-entry" itemprop="articleBody">
      
        <h2 id="Introduction">Introduction</h2><p>In machine learning and data mining applications, the data points usually lie in a high dimension space. Therefore, it’s better to perform dimension reduction to these data points. There are two widely used methods, that is PCA and LDA. </p>
<ol>
<li>PCA is an unsupervised method. It performs dimensionality reduction while preserving as much of the variance in the high dimensional space as possible. In fact, it projects data in the directions of maximum variance. However, the directions of maximum variance may be useless for classification.</li>
<li>LDA is a supervised method. It performs dimensionality reduction while preserving as much of the class discriminatory information as possible, which means that it projects to a line which preserves direction useful for data classification.</li>
</ol>
<h2 id="Main_Idea">Main Idea</h2><pre><code><span class="attribute">Objective</span>: <span class="string">maximise the between-class separability while minimising their within-class variability</span>

<span class="livecodeserver">Solution: maximizing <span class="operator">the</span> difference between <span class="operator">the</span> means, normalized <span class="keyword">by</span> <span class="operator">a</span> measure <span class="operator">of</span> <span class="operator">the</span> <span class="operator">within</span>-class scatter. </span>
</code></pre><ol>
<li><p>Two classes case<br>Given a set of D-dimensional samples ${x_1,x_2,\cdots, x_n}$, $N_1$ of which belong to class $w_1$, and $N_2$ to class $w_2$. We seek to a projection that projects the sample $x$ onto a line<br>$$<br>y = w^Tx<br>$$<br>To mesure the separation, we use the distance of the projected mean of the two classes.<br>$$<br>|\widetilde \mu_1 - \widetilde \mu_2|^2 \\<br>= |\frac{1}{N_1}\sum_{y\in w_1} y - \frac{1}{N_2}\sum_{y\in w_2} y|^2 \\<br>=|\frac{1}{N_1}\sum_{x\in w_1} w^Tx - \frac{1}{N_2}\sum_{x\in w_2}w^Tx|^2 \\<br>= |w^T(\mu_1 - \mu_2)|^2 \\<br>= w^T(\mu_1 - \mu_2)(\mu_1 - \mu_2)^Tw \\<br>= w^T S_b w<br>$$<br>where $S_b=(\mu_1 - \mu_2)(\mu_1 - \mu_2)^T$. However, the distance between projected means is not a good measure since it does not account for the standard deviation within classes, just as shown in the following figure.<br>Therefore, we define the <strong>scatter</strong> for each class.<br>$$<br>\widetilde s_i^{2} = \sum_{y\in w_i}(y-\widetilde \mu_i)^2 \\<br>= \sum_{x\in w_i}(w^Tx-w^T\mu_i)^2 \\<br>= \sum_{x\in w_i}w^T(x-\mu_i)(x-\mu_i)^Tw \\<br>= w^TS_iw<br>$$<br>where $S_i=\sum_{x\in w_i}(x-\mu_i)(x-\mu_i)^T$. Then, we have   $S_w = S_1+S_2 = \sum_{i=1}^c \sum_{x\in w_i}(x-\mu_i)(x-\mu_i)^T$.<br>After that, we have the objective function<br>$$<br>J(w) = \max_{w}\frac{w^TS_bw}{w^TS_w w}<br>$$<br>Note that $S_bx=(\mu_1-\mu_2)(\mu_1-\mu_2)^Tx=\alpha (\mu_1-\mu_2)$, thus $S_bx$ for any sample $x$, it always lies in the same direction as $(\mu_1-\mu_2)$. Therefore, we can solve the eigenvalue problem immediately $w=S_w^{-1}(\mu_1-\mu_2)$.<br>All in all, we are looking for a projection where samples from the same class are projected very close to each other and, at the same time, the projected means are as farther apart as possible.</p>
</li>
<li><p>Multiple classes case<br>For $C$-classes case, we will need $C-1$ projecting directions $W=[w_1,w_2,\cdots,w_{c-1}]$ to get $y=W^Tx$.<br>Then, the <strong>within-class scatter</strong> is as follows<br>$$<br>S_W=\sum_{i=1}^{C}S_i \<br>=\sum_{i=1}^{C} \sum_{x\in w_i} (x-\mu_i)(x-\mu_i)^T<br>$$<br>where $\mu_i=\frac{1}{N_i}\sum_{x\in w_i}x$. The <strong>between-class scatter</strong> is as follows<br>$$<br>S_B = \sum_{i=1}^{C}N_i(\mu_i-\mu)(\mu_i-\mu)^T<br>$$<br>where $\mu=\frac{1}{N}\sum x$. Note that $S_T=S_B+S_W$ is the <strong>total scatter</strong>.<br>However, $W^TS_WW$ and $W^TS_BW$ are matrix rather than scalar. To convert it as a scalar. There are two widely used methods as follows<br>$$<br>J(W) = \max_{W}\frac{|W^TS_WW|}{|W^TS_BW|}<br>$$<br>or<br>$$<br>J(W) = \max_{W}\frac{tr(W^TS_WW)}{tr(W^TS_BW)}<br>$$<br>As we know, the determinant is equal to the multiplication of the eigenvalue of the matrix, and the trace of a matrix is equal to the sum of its eigenvalue.</p>
</li>
</ol>
<ul>
<li>For the det-objective function, we can solve the generalized eigenvalue problem $S_Bw=\lambda S_Ww$ to get the solution. </li>
<li>For the trace-objective function, it is a trace ratio problem. We can solve it with Iterative Trace Ratio (ITR) method. GIven $\lambda_t$ at each iteration $t$, one searches for a transform matrix according to the trace difference as<br>$$<br>W_t = \arg\max_{W}tr(W^T(S_W-\lambda S_B)W)<br>$$<br>and renew $\lambda_{t+1}$ as the trace ratio given by $W_t$:<br>$$<br>\lambda_{t+1} =\frac{tr(W_t^TS_WW_t)}{tr(W_t^TS_BW_t)}<br>$$<br>Note that $S_B$ is the sum of $C$ matrices of $rank\leq 1$ and the mean vectors are constrained by $\frac{1}{C}\sum_{i=1}^{C}\mu_i=\mu$, therefore</li>
<li>$S_B$ will be of rank $C-1$ or less</li>
<li>This means that only $C-1$ of the eigenvalues will be non-zero.</li>
</ul>
<h2 id="Limitations">Limitations</h2><ul>
<li>LDA produces at most $C-1$ feature projections</li>
<li>LDA assumes the data points from different classes follow Gaussian distribution with same covariance. If the distributions are significantly non-Gaussian or Gaussian-with-different-covariance, the LDA projections may not preserve complex structure in the data needed for classification.</li>
</ul>

      
    </div>
    <footer class="article-footer">
      <a data-url="http://yoursite.com/2015/08/01/Linear-Disciminant-Analysis/" data-id="cictuz6kx000bqcffnib7tl1p" class="article-share-link">Share</a>
      
        <a href="http://yoursite.com/2015/08/01/Linear-Disciminant-Analysis/#disqus_thread" class="article-comment-link">Comments</a>
      
      
  <ul class="article-tag-list"><li class="article-tag-list-item"><a class="article-tag-list-link" href="/tags/Machine-Learning/">Machine Learning</a></li></ul>

    </footer>
  </div>
  
    
<nav id="article-nav">
  
  
    <a href="/2015/07/29/Subgradient-Method/" id="article-nav-older" class="article-nav-link-wrap">
      <strong class="article-nav-caption">Older</strong>
      <div class="article-nav-title">Subgradient Method</div>
    </a>
  
</nav>

  
</article>


<section id="comments">
	<div id="disqus_thread"></div>
	<script type="text/javascript">
		/* * * CONFIGURATION VARIABLES * * */
		var disqus_shortname = 'yyyoucgithubio';
		
		/* * * DON'T EDIT BELOW THIS LINE * * */
		(function() {
			var dsq = document.createElement('script'); dsq.type = 'text/javascript'; dsq.async = true;
			dsq.src = '//' + disqus_shortname + '.disqus.com/embed.js';
			(document.getElementsByTagName('head')[0] || document.getElementsByTagName('body')[0]).appendChild(dsq);
		})();
	</script>
	<noscript>Please enable JavaScript to view the <a href="https://disqus.com/?ref_noscript" rel="nofollow">comments powered by Disqus.</a></noscript>
</section>
</section>
        
          <aside id="sidebar">
  
    
  
    
  <div class="widget-wrap">
    <h3 class="widget-title">Tags</h3>
    <div class="widget">
      <ul class="tag-list"><li class="tag-list-item"><a class="tag-list-link" href="/tags/Hexo/">Hexo</a><span class="tag-list-count">1</span></li><li class="tag-list-item"><a class="tag-list-link" href="/tags/Machine-Learning/">Machine Learning</a><span class="tag-list-count">4</span></li><li class="tag-list-item"><a class="tag-list-link" href="/tags/Optimization-Method/">Optimization Method</a><span class="tag-list-count">3</span></li></ul>
    </div>
  </div>

  
    
  <div class="widget-wrap">
    <h3 class="widget-title">Tag Cloud</h3>
    <div class="widget tagcloud">
      <a href="/tags/Hexo/" style="font-size: 10px;">Hexo</a><a href="/tags/Machine-Learning/" style="font-size: 20px;">Machine Learning</a><a href="/tags/Optimization-Method/" style="font-size: 15px;">Optimization Method</a>
    </div>
  </div>

  
    
  <div class="widget-wrap">
    <h3 class="widget-title">Archives</h3>
    <div class="widget">
      <ul class="archive-list"><li class="archive-list-item"><a class="archive-list-link" href="/archives/2015/08/">August 2015</a><span class="archive-list-count">1</span></li><li class="archive-list-item"><a class="archive-list-link" href="/archives/2015/07/">July 2015</a><span class="archive-list-count">6</span></li><li class="archive-list-item"><a class="archive-list-link" href="/archives/2015/05/">May 2015</a><span class="archive-list-count">1</span></li></ul>
    </div>
  </div>

  
    
  <div class="widget-wrap">
    <h3 class="widget-title">Recents</h3>
    <div class="widget">
      <ul>
        
          <li>
            <a href="/2015/08/01/Linear-Disciminant-Analysis/">Linear Disciminant Analysis</a>
          </li>
        
          <li>
            <a href="/2015/07/29/Subgradient-Method/">Subgradient Method</a>
          </li>
        
          <li>
            <a href="/2015/07/24/Gradient-Descent-Method/">Gradient Descent Method</a>
          </li>
        
          <li>
            <a href="/2015/07/21/Convex-Function/">Convex Function</a>
          </li>
        
          <li>
            <a href="/2015/07/20/Density-Estimation/">Density Estimation</a>
          </li>
        
      </ul>
    </div>
  </div>

  
</aside>
        
		
<section id="comment">	
	<div id="disqus_thread"></div>
	<script type="text/javascript">
		/* * * CONFIGURATION VARIABLES * * */
		var disqus_shortname = 'yyyouc';
		
		/* * * DON'T EDIT BELOW THIS LINE * * */
		(function() {
			var dsq = document.createElement('script'); dsq.type = 'text/javascript'; dsq.async = true;
			dsq.src = '//' + disqus_shortname + '.disqus.com/embed.js';
			(document.getElementsByTagName('head')[0] || document.getElementsByTagName('body')[0]).appendChild(dsq);
		})();
	</script>
	<noscript>Please enable JavaScript to view the <a href="https://disqus.com/?ref_noscript" rel="nofollow">comments powered by Disqus.</a></noscript>
<a href="http://disqus.com" class="dsq-brlink">comments powered by <span class="logo-disqus">Disqus</span></a>
</section>
	
      </div>
      <footer id="footer">
  
  <div class="outer">
    <div id="footer-info" class="inner">
      &copy; 2015 YYY OUC<br>
      Powered by <a href="http://hexo.io/" target="_blank">Hexo</a>
    </div>
  </div>
</footer>
    </div>
    <nav id="mobile-nav">
  
    <a href="/" class="mobile-nav-link">Home</a>
  
    <a href="/archives" class="mobile-nav-link">Archives</a>
  
</nav>
    
<script>
  var disqus_shortname = 'yyyouc';
  
  var disqus_url = 'http://yoursite.com/2015/08/01/Linear-Disciminant-Analysis/';
  
  (function(){
    var dsq = document.createElement('script');
    dsq.type = 'text/javascript';
    dsq.async = true;
    dsq.src = '//' + disqus_shortname + '.disqus.com/embed.js';
    (document.getElementsByTagName('head')[0] || document.getElementsByTagName('body')[0]).appendChild(dsq);
  })();
</script>


<script src="//ajax.googleapis.com/ajax/libs/jquery/2.0.3/jquery.min.js"></script>


  <link rel="stylesheet" href="/fancybox/jquery.fancybox.css" type="text/css">
  <script src="/fancybox/jquery.fancybox.pack.js" type="text/javascript"></script>


<script src="/js/script.js" type="text/javascript"></script>


<script type="text/x-mathjax-config"> 
MathJax.Hub.Config({ 
  tex2jax: {inlineMath: [['$','$'], ['\\(','\\)']]} 
}); 
</script>
<script type="text/javascript"
   src="http://cdn.mathjax.org/mathjax/latest/MathJax.js?config=TeX-AMS-MML_HTMLorMML">
</script>


	


<!-- mathjax config similar to math.stackexchange -->

<script type="text/x-mathjax-config">
  MathJax.Hub.Config({
    tex2jax: {
      inlineMath: [ ['$','$'], ["\\(","\\)"] ],
      processEscapes: true
    }
  });
</script>

<script type="text/x-mathjax-config">
    MathJax.Hub.Config({
      tex2jax: {
        skipTags: ['script', 'noscript', 'style', 'textarea', 'pre', 'code']
      }
    });
</script>

<script type="text/x-mathjax-config">
    MathJax.Hub.Queue(function() {
        var all = MathJax.Hub.getAllJax(), i;
        for(i=0; i < all.length; i += 1) {
            all[i].SourceElement().parentNode.className += ' has-jax';
        }
    });
</script>

<script type="text/javascript" src="http://cdn.mathjax.org/mathjax/latest/MathJax.js?config=TeX-AMS-MML_HTMLorMML">
</script>


  </div>
</body>
</html>