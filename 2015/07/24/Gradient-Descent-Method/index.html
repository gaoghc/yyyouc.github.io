<!DOCTYPE html>
<html>
<head>
  <meta charset="utf-8">
  
  <title>Gradient Descent Method | YYYOUC</title>
  <meta name="viewport" content="width=device-width, initial-scale=1, maximum-scale=1">
  <meta name="description" content="Gradient method lays the foundation of the optimization algorithm. Thus, we will talk about its basic idea and its convergence in this work.
DefinitionIn general, we assume that 

$f(x)$ is convex and">
<meta property="og:type" content="article">
<meta property="og:title" content="Gradient Descent Method">
<meta property="og:url" content="http://yoursite.com/2015/07/24/Gradient-Descent-Method/index.html">
<meta property="og:site_name" content="YYYOUC">
<meta property="og:description" content="Gradient method lays the foundation of the optimization algorithm. Thus, we will talk about its basic idea and its convergence in this work.
DefinitionIn general, we assume that 

$f(x)$ is convex and">
<meta name="twitter:card" content="summary">
<meta name="twitter:title" content="Gradient Descent Method">
<meta name="twitter:description" content="Gradient method lays the foundation of the optimization algorithm. Thus, we will talk about its basic idea and its convergence in this work.
DefinitionIn general, we assume that 

$f(x)$ is convex and">
  
    <link rel="alternative" href="/atom.xml" title="YYYOUC" type="application/atom+xml">
  
  
    <link rel="icon" href="/favicon.png">
  
  <link href="//fonts.googleapis.com/css?family=Source+Code+Pro" rel="stylesheet" type="text/css">
  <link rel="stylesheet" href="/css/style.css" type="text/css">
  

</head>
<body>
  <div id="container">
    <div id="wrap">
      <header id="header">
  <div id="banner"></div>
  <div id="header-outer" class="outer">
    <div id="header-title" class="inner">
      <h1 id="logo-wrap">
        <a href="/" id="logo">YYYOUC</a>
      </h1>
      
    </div>
    <div id="header-inner" class="inner">
      <nav id="main-nav">
        <a id="main-nav-toggle" class="nav-icon"></a>
        
          <a class="main-nav-link" href="/">Home</a>
        
          <a class="main-nav-link" href="/archives">Archives</a>
        
      </nav>
      <nav id="sub-nav">
        
          <a id="nav-rss-link" class="nav-icon" href="/atom.xml" title="RSS Feed"></a>
        
        <a id="nav-search-btn" class="nav-icon" title="Search"></a>
      </nav>
      <div id="search-form-wrap">
        <form action="//google.com/search" method="get" accept-charset="UTF-8" class="search-form"><input type="search" name="q" results="0" class="search-form-input" placeholder="Search"><button type="submit" class="search-form-submit">&#xF002;</button><input type="hidden" name="q" value="site:http://yoursite.com"></form>
      </div>
    </div>
  </div>
</header>
      <div class="outer">
        <section id="main"><article id="post-Gradient-Descent-Method" class="article article-type-post" itemscope itemprop="blogPost">
  <div class="article-meta">
    <a href="/2015/07/24/Gradient-Descent-Method/" class="article-date">
  <time datetime="2015-07-25T02:16:54.000Z" itemprop="datePublished">2015-07-24</time>
</a>
    
  </div>
  <div class="article-inner">
    
    
      <header class="article-header">
        
  
    <h1 class="article-title" itemprop="name">
      Gradient Descent Method
    </h1>
  

      </header>
    
    <div class="article-entry" itemprop="articleBody">
      
        <p>Gradient method lays the foundation of the optimization algorithm. Thus, we will talk about its basic idea and its convergence in this work.</p>
<h2 id="Definition">Definition</h2><p>In general, we assume that </p>
<ol>
<li>$f(x)$ is convex and differentiable with $dom f = \Re^n$</li>
<li>$\nabla f(x)$ is Lipschitz continous with parameter $L&gt;0$</li>
</ol>
<p>The general form is as follows,<br>$$<br>x_{k+1}=x_{k}-t_k\nabla f(x_k)<br>$$<br>where $\nabla f(x_k)$ is the gradient, $t_k$ is the step size. There are three strategies to determine the step size.</p>
<ol>
<li>Fixed: $t_k$ is constant</li>
<li>exact line search: minimize $f(x-t\nabla f(x))$ over $t$</li>
<li>backtracking line search:</li>
</ol>
<p>The first strategy is the simplest one. Indeed, it is often used, but only in convex optimization, where the behavior of functions is much more predictable than in the general nonlinear case.</p>
<p>The second strategy is completely theoretical. It is never used in practice since even in one-dimensional case we cannot find an exact minimum of a function in finite time.</p>
<p>The third strategy is used in the majority of the practical algorithms. We will talk about this strategy in the future work.</p>
<h2 id="Convergence_of_General_Convex_Function">Convergence of General Convex Function</h2><p>Since $f$ is Lipschitz continous, therefore, for $x_{k+1}=x_{k}-t\nabla f(x)$, we have<br>$$<br>f(x_{k+1})\leq f(x_{k})-\nabla f(x)^T(x_{k+1}-x_{k}) + \frac{L}{2}||x_{k+1}-x_{k}||_2^2 = f(x_k)-t(1-\frac{Lt}{2})||\nabla f(x)||_2^2<br>$$<br>If we set $t\leq 1/L$, then<br>$$<br>f(x_{k+1})\leq f(x_{k})- \frac{t}{2} ||\nabla f(x_k)||_2^2 \\<br>\leq f(x^{opt})+ \nabla f(x_k)^T(x_k-x^{opt})- \frac{t}{2} ||\nabla f(x_k)||_2^2 \\<br>= f(x^{opt})+ \frac{1}{2t} (||x_k-x^{opt}||_2^2-||x_k-x^{opt}||_2^2 + 2t \nabla f(x_k)^T(x_k-x^{opt})-t^2 ||\nabla f(x_k)||_2^2) \\<br>= f(x^{opt})+ \frac{1}{2t} (||x_k-x^{opt}||_2^2 - ||x_k-x^{opt}-t\nabla f(x_k)||_2^2) \\<br>= f(x^{opt})+ \frac{1}{2t} (||x_k-x^{opt}||_2^2 - ||x_{k+1}-x^{opt}||_2^2)<br>$$</p>
<p>Therefore, we have<br>$$<br>\sum_{i=1}^k (f(x_i)-f(x^{opt})) \leq \frac{1}{2t}\sum_{i=1}^k (||x_{i-1}-x^{opt}||_2^2-||x_{i}-x^{opt}||_2^2) \\<br>= \frac{1}{2t} (||x_{0}-x^{opt}||_2^2-||x_{k}-x^{opt}||_2^2) \\<br>\leq \frac{1}{2t} ||x_{0}-x^{opt}||_2^2<br>$$<br>Since $f(x_i)$ is non-increasing, we have<br>$$<br>f(x_k)-f(x^{opt}) \leq \frac{1}{k} \sum_{i=1}^k (f(x_i)-f(x^{opt})) \leq \frac{1}{2kt} ||x_{0}-x^{opt}||_2^2<br>$$</p>
<p>As a result, the number of iterations to reach $f(x_k)-f(x^{opt}) \leq \epsilon$ is $O(1/ \epsilon)$.</p>
<h2 id="Convergence_of_Strongly_Convex_Function">Convergence of Strongly Convex Function</h2><p>For strongly convex function, we have<br>$$<br>f(y)\geq f(x)+\nabla f(x)^T(y-x)+\frac{\mu}{2}||y-x||_2^2<br>$$</p>
<p>If $x_{k+1}=x_{k}-t\nabla f(x)$ and 0\leq t\leq 2/(\mu+L), we have<br>$$<br>||x_{k+1}-x^{opt}||_2^2=||x_k-t \nabla f(x_k)-x^{opt}||_2^2 \\<br>= ||x_{k}-x^{opt}||_2^2 - 2t\nabla f(x_k)^T(x_k-x^{opt})+t^2||\nabla f(x_k)||_2^2 \\<br>\leq (1-t\frac{2mL}{m+L})||x_k-x^{opt}||_2^2+t(t-\frac{2}{m+L})||\nabla f(x_k)||_2^2 \\<br>\leq (1-t\frac{2mL}{m+L})||x_k-x^{opt}||_2^2<br>$$<br>Thus, $||x_k-x^{opt}||_2^2 \leq (1-t\frac{2mL}{m+L})^k ||x_0-x^{opt}||_2^2$ , which implies linear convergence.</p>
<p>Due to the quadratic upper bound of function with Lipschitz continous gradient, we have<br>$$<br>f(x_{k})-f(x^{opt}) \leq \frac{L}{2}||x_{k}-x^{opt}||_2^2 \leq \frac{c^k L}{2}||x_{0}-x^{opt}||_2^2<br>$$<br>As a result, the number of iterations to reach $f(x_k)-f(x^{opt}) \leq \epsilon$ is $O(log(1/ \epsilon))$.</p>

      
    </div>
    <footer class="article-footer">
      <a data-url="http://yoursite.com/2015/07/24/Gradient-Descent-Method/" data-id="cicjybk3c0008ysffrp90phs0" class="article-share-link">Share</a>
      
        <a href="http://yoursite.com/2015/07/24/Gradient-Descent-Method/#disqus_thread" class="article-comment-link">Comments</a>
      
      
  <ul class="article-tag-list"><li class="article-tag-list-item"><a class="article-tag-list-link" href="/tags/Optimization-Method/">Optimization Method</a></li></ul>

    </footer>
  </div>
  
    
<nav id="article-nav">
  
  
    <a href="/2015/07/21/Convex-Function/" id="article-nav-older" class="article-nav-link-wrap">
      <strong class="article-nav-caption">Older</strong>
      <div class="article-nav-title">Convex Function</div>
    </a>
  
</nav>

  
</article>


<section id="comments">
	<div id="disqus_thread"></div>
	<script type="text/javascript">
		/* * * CONFIGURATION VARIABLES * * */
		var disqus_shortname = 'yyyoucgithubio';
		
		/* * * DON'T EDIT BELOW THIS LINE * * */
		(function() {
			var dsq = document.createElement('script'); dsq.type = 'text/javascript'; dsq.async = true;
			dsq.src = '//' + disqus_shortname + '.disqus.com/embed.js';
			(document.getElementsByTagName('head')[0] || document.getElementsByTagName('body')[0]).appendChild(dsq);
		})();
	</script>
	<noscript>Please enable JavaScript to view the <a href="https://disqus.com/?ref_noscript" rel="nofollow">comments powered by Disqus.</a></noscript>
</section>
</section>
        
          <aside id="sidebar">
  
    
  
    
  <div class="widget-wrap">
    <h3 class="widget-title">Tags</h3>
    <div class="widget">
      <ul class="tag-list"><li class="tag-list-item"><a class="tag-list-link" href="/tags/Hexo/">Hexo</a><span class="tag-list-count">1</span></li><li class="tag-list-item"><a class="tag-list-link" href="/tags/Machine-Learning/">Machine Learning</a><span class="tag-list-count">3</span></li><li class="tag-list-item"><a class="tag-list-link" href="/tags/Optimization-Method/">Optimization Method</a><span class="tag-list-count">2</span></li></ul>
    </div>
  </div>

  
    
  <div class="widget-wrap">
    <h3 class="widget-title">Tag Cloud</h3>
    <div class="widget tagcloud">
      <a href="/tags/Hexo/" style="font-size: 10px;">Hexo</a><a href="/tags/Machine-Learning/" style="font-size: 20px;">Machine Learning</a><a href="/tags/Optimization-Method/" style="font-size: 15px;">Optimization Method</a>
    </div>
  </div>

  
    
  <div class="widget-wrap">
    <h3 class="widget-title">Archives</h3>
    <div class="widget">
      <ul class="archive-list"><li class="archive-list-item"><a class="archive-list-link" href="/archives/2015/07/">July 2015</a><span class="archive-list-count">5</span></li><li class="archive-list-item"><a class="archive-list-link" href="/archives/2015/05/">May 2015</a><span class="archive-list-count">1</span></li></ul>
    </div>
  </div>

  
    
  <div class="widget-wrap">
    <h3 class="widget-title">Recents</h3>
    <div class="widget">
      <ul>
        
          <li>
            <a href="/2015/07/24/Gradient-Descent-Method/">Gradient Descent Method</a>
          </li>
        
          <li>
            <a href="/2015/07/21/Convex-Function/">Convex Function</a>
          </li>
        
          <li>
            <a href="/2015/07/20/Density-Estimation/">Density Estimation</a>
          </li>
        
          <li>
            <a href="/2015/07/18/Bayesian-Decision-Theory/">Bayesian Decision Theory</a>
          </li>
        
          <li>
            <a href="/2015/07/07/Support Vector Machine/">Support Vector Machine</a>
          </li>
        
      </ul>
    </div>
  </div>

  
</aside>
        
		
<section id="comment">	
	<div id="disqus_thread"></div>
	<script type="text/javascript">
		/* * * CONFIGURATION VARIABLES * * */
		var disqus_shortname = 'yyyouc';
		
		/* * * DON'T EDIT BELOW THIS LINE * * */
		(function() {
			var dsq = document.createElement('script'); dsq.type = 'text/javascript'; dsq.async = true;
			dsq.src = '//' + disqus_shortname + '.disqus.com/embed.js';
			(document.getElementsByTagName('head')[0] || document.getElementsByTagName('body')[0]).appendChild(dsq);
		})();
	</script>
	<noscript>Please enable JavaScript to view the <a href="https://disqus.com/?ref_noscript" rel="nofollow">comments powered by Disqus.</a></noscript>
<a href="http://disqus.com" class="dsq-brlink">comments powered by <span class="logo-disqus">Disqus</span></a>
</section>
	
      </div>
      <footer id="footer">
  
  <div class="outer">
    <div id="footer-info" class="inner">
      &copy; 2015 YYY OUC<br>
      Powered by <a href="http://hexo.io/" target="_blank">Hexo</a>
    </div>
  </div>
</footer>
    </div>
    <nav id="mobile-nav">
  
    <a href="/" class="mobile-nav-link">Home</a>
  
    <a href="/archives" class="mobile-nav-link">Archives</a>
  
</nav>
    
<script>
  var disqus_shortname = 'yyyouc';
  
  var disqus_url = 'http://yoursite.com/2015/07/24/Gradient-Descent-Method/';
  
  (function(){
    var dsq = document.createElement('script');
    dsq.type = 'text/javascript';
    dsq.async = true;
    dsq.src = '//' + disqus_shortname + '.disqus.com/embed.js';
    (document.getElementsByTagName('head')[0] || document.getElementsByTagName('body')[0]).appendChild(dsq);
  })();
</script>


<script src="//ajax.googleapis.com/ajax/libs/jquery/2.0.3/jquery.min.js"></script>


  <link rel="stylesheet" href="/fancybox/jquery.fancybox.css" type="text/css">
  <script src="/fancybox/jquery.fancybox.pack.js" type="text/javascript"></script>


<script src="/js/script.js" type="text/javascript"></script>


<script type="text/x-mathjax-config"> 
MathJax.Hub.Config({ 
  tex2jax: {inlineMath: [['$','$'], ['\\(','\\)']]} 
}); 
</script>
<script type="text/javascript"
   src="http://cdn.mathjax.org/mathjax/latest/MathJax.js?config=TeX-AMS-MML_HTMLorMML">
</script>


	


<!-- mathjax config similar to math.stackexchange -->

<script type="text/x-mathjax-config">
  MathJax.Hub.Config({
    tex2jax: {
      inlineMath: [ ['$','$'], ["\\(","\\)"] ],
      processEscapes: true
    }
  });
</script>

<script type="text/x-mathjax-config">
    MathJax.Hub.Config({
      tex2jax: {
        skipTags: ['script', 'noscript', 'style', 'textarea', 'pre', 'code']
      }
    });
</script>

<script type="text/x-mathjax-config">
    MathJax.Hub.Queue(function() {
        var all = MathJax.Hub.getAllJax(), i;
        for(i=0; i < all.length; i += 1) {
            all[i].SourceElement().parentNode.className += ' has-jax';
        }
    });
</script>

<script type="text/javascript" src="http://cdn.mathjax.org/mathjax/latest/MathJax.js?config=TeX-AMS-MML_HTMLorMML">
</script>


  </div>
</body>
</html>