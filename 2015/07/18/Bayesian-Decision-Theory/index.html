<!DOCTYPE html>
<html>
<head>
  <meta charset="utf-8">
  
  <title>Bayesian Decision Theory | YYYOUC</title>
  <meta name="viewport" content="width=device-width, initial-scale=1, maximum-scale=1">
  <meta name="description" content="IntroductionBayesian decision theory is a fundamental statistical method to the problem of pattern classification. It is to use the probability to perform classification decision. It has two assumptio">
<meta property="og:type" content="article">
<meta property="og:title" content="Bayesian Decision Theory">
<meta property="og:url" content="http://yoursite.com/2015/07/18/Bayesian-Decision-Theory/index.html">
<meta property="og:site_name" content="YYYOUC">
<meta property="og:description" content="IntroductionBayesian decision theory is a fundamental statistical method to the problem of pattern classification. It is to use the probability to perform classification decision. It has two assumptio">
<meta name="twitter:card" content="summary">
<meta name="twitter:title" content="Bayesian Decision Theory">
<meta name="twitter:description" content="IntroductionBayesian decision theory is a fundamental statistical method to the problem of pattern classification. It is to use the probability to perform classification decision. It has two assumptio">
  
    <link rel="alternative" href="/atom.xml" title="YYYOUC" type="application/atom+xml">
  
  
    <link rel="icon" href="/favicon.png">
  
  <link href="//fonts.googleapis.com/css?family=Source+Code+Pro" rel="stylesheet" type="text/css">
  <link rel="stylesheet" href="/css/style.css" type="text/css">
  

</head>
<body>
  <div id="container">
    <div id="wrap">
      <header id="header">
  <div id="banner"></div>
  <div id="header-outer" class="outer">
    <div id="header-title" class="inner">
      <h1 id="logo-wrap">
        <a href="/" id="logo">YYYOUC</a>
      </h1>
      
    </div>
    <div id="header-inner" class="inner">
      <nav id="main-nav">
        <a id="main-nav-toggle" class="nav-icon"></a>
        
          <a class="main-nav-link" href="/">Home</a>
        
          <a class="main-nav-link" href="/archives">Archives</a>
        
      </nav>
      <nav id="sub-nav">
        
          <a id="nav-rss-link" class="nav-icon" href="/atom.xml" title="RSS Feed"></a>
        
        <a id="nav-search-btn" class="nav-icon" title="Search"></a>
      </nav>
      <div id="search-form-wrap">
        <form action="//google.com/search" method="get" accept-charset="UTF-8" class="search-form"><input type="search" name="q" results="0" class="search-form-input" placeholder="Search"><button type="submit" class="search-form-submit">&#xF002;</button><input type="hidden" name="q" value="site:http://yoursite.com"></form>
      </div>
    </div>
  </div>
</header>
      <div class="outer">
        <section id="main"><article id="post-Bayesian-Decision-Theory" class="article article-type-post" itemscope itemprop="blogPost">
  <div class="article-meta">
    <a href="/2015/07/18/Bayesian-Decision-Theory/" class="article-date">
  <time datetime="2015-07-19T01:38:12.000Z" itemprop="datePublished">2015-07-18</time>
</a>
    
  </div>
  <div class="article-inner">
    
    
      <header class="article-header">
        
  
    <h1 class="article-title" itemprop="name">
      Bayesian Decision Theory
    </h1>
  

      </header>
    
    <div class="article-entry" itemprop="articleBody">
      
        <h2 id="Introduction">Introduction</h2><p>Bayesian decision theory is a fundamental statistical method to the problem of pattern classification. It is to use the probability to perform classification decision. It has two assumptions:</p>
<ul>
<li>the decision problem is posed in probilistic terms;</li>
<li>all the involved probability values are known.</li>
</ul>
<h2 id="Bayes_Formula">Bayes Formula</h2><p>Bayes formula is the basis of the Bayesian decision theory. It is defined as follows:<br>$$<br>P(w_j|x)=\frac{p(x|w_j)P(w_j)}{p(x)}<br>$$<br>where $P(w_j|x)$ is the <strong>posterior probability</strong>, $P(w_j)$ is the <strong>prior probability</strong> and $p(x|w_j)$ is the <strong>conditional probability density</strong>. And $p(x)=\sum_{j=1}^{n}p(x|w_j)P(w_j)$.<br>But what is the meaning of prior probability, posterior probability and the conditional probability density?</p>
<ul>
<li>Prior probability is the assumption that we give to a problem. It may be correct or not. However, we can use conditional probability density to justify it.</li>
<li>Conditional probability density is the likelihood of $w_j$ with respect to $x$. If it is large, it means that $w_j$ is more “likely” to be the true category.</li>
<li>Posterior probability is the justified assumption. </li>
</ul>
<p>In fact, given a data point, we can give an assumption about the category which it belongs to. Such assumption maybe right or not. However, such a data point has some features. Therefore, under such a assumption, we can use these features of the data point to justify whether the assumtion is proper. If so, we will get a large posterior probability. Otherwise, we will get a small one.<br>This is the basic idea of Bayes formula. It can be interpreted as follows:<br>$$<br>posterior = \frac{likelihood\times prior}{evidence}<br>$$<br>This formula shows that by observing the value of $x$ we can convert the prior probability $P(w_j)$to the a  posterior probability $P(w_j|x)$.</p>
<h2 id="Loss_Function">Loss Function</h2><p>Now we can use Bayes formula to give decision about the classification. But how to measure the cost of our decision?</p>
<p>Since $P(w_j|x)$ is the probability that the true state of nature is $w_j$ , the expected loss associated with taking action $\alpha_i$ is<br>$$<br>L(\alpha_i|x)=\sum_{j=1}^{c}I(\alpha_i|w_j)P(w_j|x)<br>$$<br>In decision-theoretic terminology, an expected loss is called a <strong>risk</strong>, and $L(\alpha_i|x)$ is called the <strong>conditional risk</strong>. Whenever we encounter a particular observation $x$, we can minimize our expected loss by selecting the action that minimizes the conditional risk. Since $L(\alpha_i|x)$ is the conditional risk, <strong>overall risk</strong> is given by<br>$$<br>L=\int L(\alpha(x)|x)p(x)dx<br>$$<br>Clearly, if $\alpha(x)$ is chosen such that $L(\alpha(x))$ is as small as possible for every $x$, then the overall risk will be minimized. This justifies the following statement of the Bayes decision rule: To minimize the overall risk, compute the conditional risk<br>$$<br>L(\alpha_i|x)=\sum_{j=1}^{c}I(\alpha_i|w_j)P(w_j|x)<br>$$<br>and select the action $\alpha_i$ for which $L(\alpha_i|x)$ is minimum. The resulting minimum overall risk is called the <strong>Bayes risk</strong>.</p>
<p>Now, let us define a function<br>$$<br>I(\alpha_j|w_i)=\left\{<br>\begin{aligned}<br>0 &amp;&amp; i=j \\<br>1 &amp;&amp; i \neq j \\<br>\end{aligned}<br>\right.<br>$$<br>Then, the conditional risk can be defined as follows.<br>$$<br>L(\alpha_i|x)=\sum_{j=1}^{c}I(\alpha_i|w_j)P(w_j|x) \<br>= \sum_{j\neq i}P(w_j|x) \<br>= 1-P(w_i|x)<br>$$<br>From the definition of the loss function, we can conclude that minimizing loss function $L(\alpha_i|x)$ is to maximize the posterior probability $P(w_j|x)$. In other words, decide $w_i$ if $P(w_i|x)&gt;P(w_j|x)$ for all $j\neq i$</p>
<h2 id="Bayes_Classifier">Bayes Classifier</h2><p>There are many different ways to represent pattern classifiers. One of the most useful is in terms of a set of <strong>discriminant functions</strong> $g_i(x), i = 1\cdots c$. The classifier is said to assign a data point $x$ to class $w_i$ if for all $j\neq i$<br>$$<br>g_i(x)&gt;g_j(x)<br>$$</p>
<p>A Bayes classifier is easily and naturally represented in this way. For the general case with risks, we can let $g_i(x) = -L(\alpha_i|x)$, since the maximum discriminant function will then correspond to the minimum conditional risk. For the minimum-error-rate case, we can simplify things further by taking $g_i(x) = P(w_j|x)$, so that the maximum discriminant function corresponds to the maximum posterior probability.</p>
<p>Clearly, the choice of discriminant functions is not unique. We can always multiply all the discriminant functions by the same positive constant or shift them by the same additive constant without influencing the decision. More generally, if we replace every $g_i(x)$ by $f(g_i(x))$, where $f(.)$ is a monotonically increasing function, the resulting classification is unchanged. This observation can lead to significant analytical and computational simplifications. In particular, for minimum-error-rate classification, any of the following choices gives identical classification results, but some can be<br>much simpler to understand or to compute than others:<br>$$<br>g_i(x)=P(w_i|x) \\<br>g_i(x) = p(x|w_i)P(w_i) \\<br>g_i(x) = lnp(x|w_i)+lnP(w_i)<br>$$</p>
<p>Even though the discriminant functions can be written in a variety of forms, the decision rules are equivalent. The effect of any decision rule is to divide the feature space into $c$ decision regions, $R_1,\cdots, R_c$. If $g_i(x) &gt; g_j(x)$ for all $j\neq i$, then $x$ is in region $R_i$, and the decision rule calls for us to assign $x$ to $w_i$. The regions are separated<br>by decision boundaries, surfaces in feature space where ties occur among the largest discriminant functions.</p>
<h2 id="Naïve_Bayes_Method">Naïve Bayes Method</h2><p>Naïve Bayes is a subset of Bayesian decision theory. It uses the Bayesian decision theory to do classification. Here, Naïve means the features of the data point are independent to each other.</p>
<p>Given a data point $x=(x_1, x_2,\cdots,x_n)$ where $x_i$ is the $i$-th feature of the data point, and given classes $c_1, c_2, \cdots, c_k$. If $P(c_i|x)&gt;P(c_j|x)$ for all $j\neq i$, then data point $x$ belongs to class $c_i$. Specifically,<br>$$<br>P(c_i|x)=\frac{p(x|c_i)P(c_i)}{p(x)}<br>$$<br>Due to the independence of the features, $p(x|c_i)=p(x_1|c_i)p(x_2|c_i)\cdots p(x_n|c_i)$.</p>
<p>This is the basic idea of Naïve Bayes method.</p>

      
    </div>
    <footer class="article-footer">
      <a data-url="http://yoursite.com/2015/07/18/Bayesian-Decision-Theory/" data-id="ciccri51f0000tsff5t2vz4fg" class="article-share-link">Share</a>
      
        <a href="http://yoursite.com/2015/07/18/Bayesian-Decision-Theory/#disqus_thread" class="article-comment-link">Comments</a>
      
      
  <ul class="article-tag-list"><li class="article-tag-list-item"><a class="article-tag-list-link" href="/tags/Machine-Learning/">Machine Learning</a></li></ul>

    </footer>
  </div>
  
    
<nav id="article-nav">
  
    <a href="/2015/07/20/Density-Estimation/" id="article-nav-newer" class="article-nav-link-wrap">
      <strong class="article-nav-caption">Newer</strong>
      <div class="article-nav-title">
        
          Density Estimation
        
      </div>
    </a>
  
  
    <a href="/2015/07/07/Support Vector Machine/" id="article-nav-older" class="article-nav-link-wrap">
      <strong class="article-nav-caption">Older</strong>
      <div class="article-nav-title">Support Vector Machine</div>
    </a>
  
</nav>

  
</article>


<section id="comments">
	<div id="disqus_thread"></div>
	<script type="text/javascript">
		/* * * CONFIGURATION VARIABLES * * */
		var disqus_shortname = 'yyyoucgithubio';
		
		/* * * DON'T EDIT BELOW THIS LINE * * */
		(function() {
			var dsq = document.createElement('script'); dsq.type = 'text/javascript'; dsq.async = true;
			dsq.src = '//' + disqus_shortname + '.disqus.com/embed.js';
			(document.getElementsByTagName('head')[0] || document.getElementsByTagName('body')[0]).appendChild(dsq);
		})();
	</script>
	<noscript>Please enable JavaScript to view the <a href="https://disqus.com/?ref_noscript" rel="nofollow">comments powered by Disqus.</a></noscript>
</section>
</section>
        
          <aside id="sidebar">
  
    
  
    
  <div class="widget-wrap">
    <h3 class="widget-title">Tags</h3>
    <div class="widget">
      <ul class="tag-list"><li class="tag-list-item"><a class="tag-list-link" href="/tags/Hexo/">Hexo</a><span class="tag-list-count">1</span></li><li class="tag-list-item"><a class="tag-list-link" href="/tags/Machine-Learning/">Machine Learning</a><span class="tag-list-count">3</span></li></ul>
    </div>
  </div>

  
    
  <div class="widget-wrap">
    <h3 class="widget-title">Tag Cloud</h3>
    <div class="widget tagcloud">
      <a href="/tags/Hexo/" style="font-size: 10px;">Hexo</a><a href="/tags/Machine-Learning/" style="font-size: 20px;">Machine Learning</a>
    </div>
  </div>

  
    
  <div class="widget-wrap">
    <h3 class="widget-title">Archives</h3>
    <div class="widget">
      <ul class="archive-list"><li class="archive-list-item"><a class="archive-list-link" href="/archives/2015/07/">July 2015</a><span class="archive-list-count">3</span></li><li class="archive-list-item"><a class="archive-list-link" href="/archives/2015/05/">May 2015</a><span class="archive-list-count">1</span></li></ul>
    </div>
  </div>

  
    
  <div class="widget-wrap">
    <h3 class="widget-title">Recents</h3>
    <div class="widget">
      <ul>
        
          <li>
            <a href="/2015/07/20/Density-Estimation/">Density Estimation</a>
          </li>
        
          <li>
            <a href="/2015/07/18/Bayesian-Decision-Theory/">Bayesian Decision Theory</a>
          </li>
        
          <li>
            <a href="/2015/07/07/Support Vector Machine/">Support Vector Machine</a>
          </li>
        
          <li>
            <a href="/2015/05/17/hello-world/">Hello World</a>
          </li>
        
      </ul>
    </div>
  </div>

  
</aside>
        
		
<section id="comment">	
	<div id="disqus_thread"></div>
	<script type="text/javascript">
		/* * * CONFIGURATION VARIABLES * * */
		var disqus_shortname = 'yyyouc';
		
		/* * * DON'T EDIT BELOW THIS LINE * * */
		(function() {
			var dsq = document.createElement('script'); dsq.type = 'text/javascript'; dsq.async = true;
			dsq.src = '//' + disqus_shortname + '.disqus.com/embed.js';
			(document.getElementsByTagName('head')[0] || document.getElementsByTagName('body')[0]).appendChild(dsq);
		})();
	</script>
	<noscript>Please enable JavaScript to view the <a href="https://disqus.com/?ref_noscript" rel="nofollow">comments powered by Disqus.</a></noscript>
<a href="http://disqus.com" class="dsq-brlink">comments powered by <span class="logo-disqus">Disqus</span></a>
</section>
	
      </div>
      <footer id="footer">
  
  <div class="outer">
    <div id="footer-info" class="inner">
      &copy; 2015 YYY OUC<br>
      Powered by <a href="http://hexo.io/" target="_blank">Hexo</a>
    </div>
  </div>
</footer>
    </div>
    <nav id="mobile-nav">
  
    <a href="/" class="mobile-nav-link">Home</a>
  
    <a href="/archives" class="mobile-nav-link">Archives</a>
  
</nav>
    
<script>
  var disqus_shortname = 'yyyouc';
  
  var disqus_url = 'http://yoursite.com/2015/07/18/Bayesian-Decision-Theory/';
  
  (function(){
    var dsq = document.createElement('script');
    dsq.type = 'text/javascript';
    dsq.async = true;
    dsq.src = '//' + disqus_shortname + '.disqus.com/embed.js';
    (document.getElementsByTagName('head')[0] || document.getElementsByTagName('body')[0]).appendChild(dsq);
  })();
</script>


<script src="//ajax.googleapis.com/ajax/libs/jquery/2.0.3/jquery.min.js"></script>


  <link rel="stylesheet" href="/fancybox/jquery.fancybox.css" type="text/css">
  <script src="/fancybox/jquery.fancybox.pack.js" type="text/javascript"></script>


<script src="/js/script.js" type="text/javascript"></script>


<script type="text/x-mathjax-config"> 
MathJax.Hub.Config({ 
  tex2jax: {inlineMath: [['$','$'], ['\\(','\\)']]} 
}); 
</script>
<script type="text/javascript"
   src="http://cdn.mathjax.org/mathjax/latest/MathJax.js?config=TeX-AMS-MML_HTMLorMML">
</script>


	


<!-- mathjax config similar to math.stackexchange -->

<script type="text/x-mathjax-config">
  MathJax.Hub.Config({
    tex2jax: {
      inlineMath: [ ['$','$'], ["\\(","\\)"] ],
      processEscapes: true
    }
  });
</script>

<script type="text/x-mathjax-config">
    MathJax.Hub.Config({
      tex2jax: {
        skipTags: ['script', 'noscript', 'style', 'textarea', 'pre', 'code']
      }
    });
</script>

<script type="text/x-mathjax-config">
    MathJax.Hub.Queue(function() {
        var all = MathJax.Hub.getAllJax(), i;
        for(i=0; i < all.length; i += 1) {
            all[i].SourceElement().parentNode.className += ' has-jax';
        }
    });
</script>

<script type="text/javascript" src="http://cdn.mathjax.org/mathjax/latest/MathJax.js?config=TeX-AMS-MML_HTMLorMML">
</script>


  </div>
</body>
</html>